{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS360 Audio Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaylil/Audio-Speech-Separation/blob/master/APS360_Audio_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1VrWW81Da5a",
        "colab_type": "code",
        "outputId": "19498b9f-8f13-4838-ac68-3cd96d70ba8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!pip install pydub\n",
        "!pip install sphfile\n",
        "!pip install wavio"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.23.1\n",
            "Collecting sphfile\n",
            "  Downloading https://files.pythonhosted.org/packages/60/f7/13771f2273bf187e4b379133450bc33a1b89fba194f20e4a29d964b6d983/sphfile-1.0.1.tar.gz\n",
            "Building wheels for collected packages: sphfile\n",
            "  Building wheel for sphfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sphfile: filename=sphfile-1.0.1-cp36-none-any.whl size=3720 sha256=16846f7b07877393f9b7b7c0e4250ef740fc42e2f5478b797ee18b0c711dbc85\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/a8/71/e339da6bfe707de7c5d1631750b163f36b6ec14efe48f97ec1\n",
            "Successfully built sphfile\n",
            "Installing collected packages: sphfile\n",
            "Successfully installed sphfile-1.0.1\n",
            "Collecting wavio\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/98/8bf5ea39a3385cc806ba1146a280a113835e5df3b0ad25ca95eea8352040/wavio-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from wavio) (1.16.4)\n",
            "Installing collected packages: wavio\n",
            "Successfully installed wavio-0.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukP8b6heuH2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "import imp\n",
        "import random\n",
        "import wavio\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import signal\n",
        "from sphfile import SPHFile\n",
        "# from chamfer_distance import ChamferDistance\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from pydub import AudioSegment\n",
        "\n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrbN1rWNgkDL",
        "colab_type": "code",
        "outputId": "283d78c6-4de2-4281-a0ec-d0acb25cba8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1KboATJQ7Ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createDictionarOnlyFirst(directory):\n",
        "    #This function takes in a path which contains overlaid files and outputs a \n",
        "    #dictionary with Person1 as a key and the path of the overlaid file as the value\n",
        "    dictionary = {}\n",
        "    \n",
        "    for filename in os.listdir(directory):\n",
        "        _, person1, _, dialect2, person2, _ = getOverlayFileInfo(filename)\n",
        "        path1, path2 = getPathsFromOverlayFile(filename)\n",
        "        if (person1 not in dictionary):\n",
        "            dictionary[person1] = []\n",
        "            dictionary[person1].append(path2)\n",
        "        else:\n",
        "            dictionary[person1].append(path2)\n",
        "    return dictionary\n",
        "  \n",
        "def move():\n",
        "  #This function is for moving all 100,000 files into 10 separate folders, onee at a time.\n",
        "  os.mkdir(\"/content/gdrive/My Drive/APS360 Project/Data/Files 9\")\n",
        "  i = 0\n",
        "  for file in os.listdir(\"/content/gdrive/My Drive/APS360 Project/Data/Generated Dataset/\"):\n",
        "    if i > 9307:\n",
        "      break\n",
        "    if i%500 == 0:\n",
        "      currentDT = datetime.datetime.now()\n",
        "      print(i, \" Time -- \", currentDT.hour, \":\", currentDT.minute)\n",
        "    oldPath = \"/content/gdrive/My Drive/APS360 Project/Data/Generated Dataset/\" + file\n",
        "    newPath = \"/content/gdrive/My Drive/APS360 Project/Data/Files 9\"\n",
        "    shutil.move(oldPath, newPath)\n",
        "    i+=1\n",
        "\n",
        "def nameOverlayFile(path1, path2):    \n",
        "    #This function takes two file paths and outputs the name of their overlay\n",
        "    dialect1, person1, sentence1 = getFileInfo(path1)\n",
        "    dialect2, person2, sentence2 = getFileInfo(path2)\n",
        "    new_file_name = dialect1 + \"_\" + person1 + \"_\" + sentence1 + \"_\" + dialect2 + \"_\" + person2 + \"_\" + sentence2\n",
        "    return new_file_name\n",
        "    \n",
        "def getFileInfo(path):\n",
        "    #This function take a path of a file and returns its details by splitting the file's name\n",
        "    tokens = path.split(\"/\")\n",
        "    dialect = tokens[-3]\n",
        "    person = tokens[-2]\n",
        "    sentence = tokens[-1].split(\".\")[0]\n",
        "    return dialect, person, sentence\n",
        "  \n",
        "def getOverlayFileInfo(path):\n",
        "    #This function takes the path of an overlay file and returns its details by splitting its name\n",
        "    tokens = path.split(\"_\")\n",
        "    # return strings in the order of: dialect1, person1, sentence1,dialect2, person2, sentence2\n",
        "    return tokens[-6], tokens[-5], tokens[-4], tokens[-3], tokens[-2], tokens[-1].split(\".\")[0]\n",
        "    \n",
        "def getPathsFromOverlayFile(oFileName, directory=\"/content/gdrive/My Drive/APS360 Project/Data/data/lisa/data/timit/raw/TIMIT/TRAIN/\"):\n",
        "    #This function takes in an overlay file's name and returns the paths of the two files it was generated from\n",
        "    #oFileName can be either the name or path (with \"/\" at the end) of the overlay file\n",
        "    dialect1, person1, sentence1, dialect2, person2, sentence2 = getOverlayFileInfo(oFileName)\n",
        "    path1 = directory + dialect1 + \"/\" + person1 + \"/\" + sentence1 + \".WAV\"\n",
        "    path2 = directory + dialect2 + \"/\" + person2 + \"/\" + sentence2 + \".WAV\"\n",
        "    return path1, path2\n",
        "    \n",
        "def getSpeakers(oFileName):\n",
        "    #This function takes in an overlay file's name and returns the two people's IDs that were used to generate it\n",
        "    tokens = oFileName.split(\"_\")\n",
        "    person1 = tokens[1]\n",
        "    person2 = tokens[4]\n",
        "    return person1, person2\n",
        "\n",
        "\n",
        "def createDictionary(directory):\n",
        "    #This function takes in a path to overlaid files and returns a dictionary with \n",
        "    #people as keys and every overlay file they were part of as their values \n",
        "    dictionary = {}\n",
        "    \n",
        "    for filename in os.listdir(directory):\n",
        "        person1, person2 = getSpeakers(filename)\n",
        "        path1, path2 = getPathsFromOverlayFile(filename)\n",
        "        if (person1 not in dictionary):\n",
        "            dictionary[person1] = []\n",
        "            dictionary[person1].append(path2)\n",
        "        else:\n",
        "            dictionary[person1].append(path2)\n",
        "        if (person2 not in dictionary):\n",
        "            dictionary[person2] = []\n",
        "            dictionary[person2].append(path1)\n",
        "        else:\n",
        "            dictionary[person2].append(path1)\n",
        "    return dictionary\n",
        "    \n",
        "#Overlay Generation\n",
        "\n",
        "#check short vs long wav files\n",
        "#overlay short on long \n",
        "#Make File Name\n",
        "#export it to a folder using file name\n",
        "#make dictionary\n",
        "#export dictionary to file\n",
        "\n",
        "def GenerateOverlay(wav1_Path, wav2_Path):\n",
        "  #This function takes two paths to wav files and generates and exports an overlay file\n",
        "  sound1 = AudioSegment.from_file(wav1_Path)\n",
        "  sound2 = AudioSegment.from_file(wav2_Path)\n",
        "  if len(sound1) > len(sound2):\n",
        "    overlaied = sound1.overlay(sound2)\n",
        "  else: \n",
        "    overlaied = sound2.overlay(sound1)\n",
        "    \n",
        "  file_handle = overlaied.export(\"/content/gdrive/My Drive/APS360 Project/Data/Generated Dataset/\" + nameOverlayFile(wav1_Path, wav2_Path), format=\"wav\")\n",
        "  return \n",
        "\n",
        "def MainGenerateData(main_path):\n",
        "  #This function generated all of our data. It was called twice, once for the training set and once for the testing set\n",
        "  people_paths = []\n",
        "  for DRx in os.listdir(train_path):                        #loop over everyone in the set\n",
        "    for person in os.listdir(train_path + DRx):             \n",
        "      people_paths.append(train_path + DRx + '/' + person)  #append every person's path to a list\n",
        "  print(\"All people paths collected...\")\n",
        "\n",
        "  person_to_wavs = dict()                             #dictionary with people's paths as keys, and wav file as values\n",
        "  for person_path in people_paths:                    #for every person in the data\n",
        "    person_wavs = []                                  #empty list for this person's WAV files\n",
        "    for file in os.listdir(person_path):              #for all this peron's files\n",
        "      if file.endswith(\"WAV\"):                        #find all the WAV files\n",
        "        person_wavs.append(person_path + '/' + file)  #add all the WAV file paths to a list\n",
        "    person_to_wavs[person_path] = person_wavs         #{ person's path:[list of their WAV paths] }\n",
        "  print(\"Person to WAVs dictionary complete...\")\n",
        "  \n",
        "  total_files_generated = 0\n",
        "  for i in range(0, len(people_paths)):                              #for every person\n",
        "    files_generated_for_this_person = 0\n",
        "    random_wav_i = random.choice(person_to_wavs[people_paths[i]])    #randomly select one of their wav files\n",
        "    for j in range(i+1, len(people_paths)):                          #for the remaining people\n",
        "      random_wav_j = random.choice(person_to_wavs[people_paths[j]])  #randomly select one of their wav files\n",
        "      GenerateOverlay(random_wav_i, random_wav_j)                    #Overlay one wav on the other\n",
        "      files_generated_for_this_person += 1\n",
        "    total_files_generated += files_generated_for_this_person\n",
        "    print(\"Completed person \", i+1, \" | Files Generated: \", files_generated_for_this_person, \" | Total Files: \", total_files_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJKCNtBVgmDS",
        "colab_type": "code",
        "outputId": "2673b602-12fc-4feb-a400-1caa59f90cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "len(os.listdir(\"/content/gdrive/My Drive/APS360 Project/Data/Files 1\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-757332ab8b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/APS360 Project/Data/Files 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/APS360 Project/Data/Files 1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwmhu3SiOl_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataFile(Dataset):\n",
        "  def __init__(self, data_folder, transform=None): #dataset = list of paths for overlaid\n",
        "    self.__xs = [] # list of paths from content to .wav for overlaid\n",
        "    self.__ys = [] # list of tuples for the ground truths for each overlaid\n",
        "    for file in os.listdir(data_folder):\n",
        "        self.__xs.append(data_folder + \"/\" + file)\n",
        "        path1, path2 = getPathsFromOverlayFile(file, directory=\"/content/gdrive/My Drive/APS360 Project/Data/data/lisa/data/timit/raw/TIMIT/TRAIN/\")\n",
        "        self.__ys.append((path1, path2))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    output1 = self.__xs[index]\n",
        "    truth = self.__ys[index]\n",
        "    return output1, truth\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.__xs)\n",
        "\n",
        "\n",
        "# data_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files 1\")\n",
        "# train_loader = DataLoader(data_set, batch_size=1, shuffle=False, num_workers=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kid24m-JSK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matchSize(a, b):\n",
        "  result = torch.zeros(b.shape)\n",
        "  result[:a.shape[0],:a.shape[1],:a.shape[2]] = a\n",
        "  return result\n",
        "\n",
        "def matchAll(a, b, c,d):\n",
        "  lens = np.array([a.shape[2], b.shape[2], c.shape[2], d.shape[2]])\n",
        "  largest_index = lens.argmax()\n",
        "  all_inputs = [a,b,c,d]\n",
        "  a = matchSize(a, all_inputs[largest_index])\n",
        "  b = matchSize(b, all_inputs[largest_index])\n",
        "  c = matchSize(c, all_inputs[largest_index])\n",
        "  d = matchSize(d, all_inputs[largest_index])\n",
        "  return a,b,c,d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WoQtfAM-mpW",
        "colab_type": "code",
        "outputId": "6ae40d73-9c0e-45e8-fcec-4ac462f78e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "#This code loades the overfitting data and prints out the length of the dataset (1 since we only have one file)\n",
        "\n",
        "data_set1 = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/One File\")\n",
        "train_loader = DataLoader(data_set1, batch_size=1, shuffle=False, num_workers=0)\n",
        "len(data_set1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-86f2e42eb4be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_set1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/One File\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1933cff1456b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder, transform)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# list of paths from content to .wav for overlaid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# list of tuples for the ground truths for each overlaid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__xs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpath1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPathsFromOverlayFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/APS360 Project/Data/data/lisa/data/timit/raw/TIMIT/TRAIN/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/One File'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjkE93m15qx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KhaledNet1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(KhaledNet1, self).__init__()\n",
        "    self.name = \"khalednet1\"\n",
        "    self.conv1 = nn.Conv2d(1, 50, 3, padding = 1) # 1 in-channel, 5 out-channels (to learn more), kernel size 3, padding 1 to keep the same size\n",
        "    self.conv2 = nn.Conv2d(50, 30, 3, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(30, 10, 3, padding = 1)\n",
        "    self.conv4 = nn.Conv2d(10, 2, 3, padding = 1) # 5 in-channel, 2 out-channels (one channel per mask?), kernel size 3, padding 1 to keep the same size\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))         # using reLU as my nonlinearity\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    output = F.relu(self.conv4(x))\n",
        "    return output\n",
        "\n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()                                                  #I was using Chamfer Loss, but I can't figure out how to import it ~ using MSELoss instead for noww\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)             #using Adam\n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:         #Loop over the batch\n",
        "      batch = 0      #initialize this batch's loss\n",
        "      for i in range(len(batchData)):                                               #for very file path in the batch\n",
        "        inputs = signal.stft(SPHFile(batchData[i]).content)[2].astype('int64')          #import and convert the overlay file to an STFT\n",
        "        truths_stft_1 = signal.stft(SPHFile(truth1[i]).content)[2].astype('int64')  #import and convert the first ground truth file to an STFT\n",
        "        truths_stft_2 = signal.stft(SPHFile(truth2[i]).content)[2].astype('int64')  #import and convert the second ground truth file to an STFT\n",
        "        \n",
        "      inputs = torch.tensor(inputs).unsqueeze(0).unsqueeze(0).float()\n",
        "      truths_stft_1 = torch.tensor(truths_stft_1).float()\n",
        "      truths_stft_2 = torch.tensor(truths_stft_2).float()\n",
        "      \n",
        "      pred = model(inputs)\n",
        "      \n",
        "      pred1 = pred[0,0,:,:]\n",
        "      pred2 = pred[0,1,:,:]\n",
        "      \n",
        "      pred1, pred2, truths_stft_1, truths_stft_2 = matchAll(pred1, pred2, truths_stft_1, truths_stft_2)\n",
        "      \n",
        "#       print(truths_stft_1.shape)\n",
        "#       print(truths_stft_2.shape)\n",
        "#       print(pred1.shape)\n",
        "#       print(pred2.shape)\n",
        "      \n",
        "      loss1 = criterion(pred1, truths_stft_1)\n",
        "      loss2 = criterion(pred2, truths_stft_2)\n",
        "      \n",
        "      loss3 = criterion(pred1, truths_stft_2)\n",
        "      loss4 = criterion(pred2, truths_stft_1)\n",
        "      \n",
        "      loss_loss = min(loss1+loss2 , loss3+loss4)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss_loss.backward()\n",
        "      optimizer.step()\n",
        "    print(loss_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrWsTH5L-brO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import signal\n",
        "from sphfile import SPHFile\n",
        "\n",
        "class KhaledNet2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(KhaledNet2, self).__init__()\n",
        "    self.name = \"khalednet2\"\n",
        "    self.conv1 = nn.Conv2d(2, 50, 3, padding = 1) \n",
        "    self.conv2 = nn.Conv2d(50, 30, 3, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(30, 10, 3, padding = 1)\n",
        "    self.conv4 = nn.Conv2d(10, 4, 3, padding = 1) \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))         # using reLU as my nonlinearity\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    output = F.relu(self.conv4(x))\n",
        "    return output\n",
        "  \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()                                                  #I was using Chamfer Loss, but I can't figure out how to import it ~ using MSELoss instead for noww\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)             #using Adam\n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:         #Loop over the batch\n",
        "      batch = 0      #initialize this batch's loss\n",
        "      for i in range(len(batchData)):                                               #for very file path in the batch\n",
        "        inputs = signal.stft(SPHFile(batchData[i]).content)[2]\n",
        "        truths_stft_1 = signal.stft(SPHFile(truth1[i]).content)[2]\n",
        "        truths_stft_2 = signal.stft(SPHFile(truth2[i]).content)[2]\n",
        "        \n",
        "      inputs = torch.tensor([inputs.real, inputs.imag]).unsqueeze(0)\n",
        "      truths1 = torch.tensor([truths_stft_1.real, truths_stft_1.imag])\n",
        "      truths2 = torch.tensor([truths_stft_2.real, truths_stft_2.imag])\n",
        "      \n",
        "      pred = model(inputs)\n",
        "      \n",
        "      pred1 = pred[0,:2,:,:]\n",
        "      pred2 = pred[0,2:,:,:]\n",
        "      \n",
        "      pred1, pred2, truths1, truths2 = matchAll(pred1, pred2, truths1, truths2)\n",
        "      \n",
        "#       print(truths_stft_1.shape)\n",
        "#       print(truths_stft_2.shape)\n",
        "#       print(pred1.shape)\n",
        "#       print(pred2.shape)\n",
        "      \n",
        "      loss1 = criterion(pred1, truths1)\n",
        "      loss2 = criterion(pred2, truths2)\n",
        "      \n",
        "      loss3 = criterion(pred1, truths2)\n",
        "      loss4 = criterion(pred2, truths1)\n",
        "      \n",
        "      loss_loss = min(loss1+loss2 , loss3+loss4)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss_loss.backward()\n",
        "      optimizer.step()\n",
        "    print(loss_loss)\n",
        "    \n",
        "model = KhaledNet2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCisTZ5pakpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, train_loader, train_loader, num_epochs = 100, learning_rate=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CycaAGF2U2be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import signal\n",
        "from sphfile import SPHFile\n",
        "\n",
        "class KhaledNet3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(KhaledNet3, self).__init__()\n",
        "    self.name = \"khalednet3\"\n",
        "    self.conv1 = nn.Conv2d(2, 50, 3, padding = 1) \n",
        "    self.conv2 = nn.Conv2d(50, 30, 3, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(30, 10, 3, padding = 1)\n",
        "    self.conv4 = nn.Conv2d(10, 4, 3, padding = 1) \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))         # using reLU as my nonlinearity\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.relu(self.conv3(x))\n",
        "    output = F.relu(self.conv4(x))\n",
        "    return output\n",
        "  \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()                                                  #I was using Chamfer Loss, but I can't figure out how to import it ~ using MSELoss instead for noww\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)             #using Adam\n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:         #Loop over the batch\n",
        "      batch = 0      #initialize this batch's loss\n",
        "      for i in range(len(batchData)):                                               #for very file path in the batch\n",
        "        inputs = signal.stft(SPHFile(batchData[i]).content)[2]\n",
        "        truths_stft_1 = signal.stft(SPHFile(truth1[i]).content)[2]\n",
        "        truths_stft_2 = signal.stft(SPHFile(truth2[i]).content)[2]\n",
        "\n",
        "      inputs = torch.tensor([inputs.real, inputs.imag]).unsqueeze(0)\n",
        "      truths1 = torch.tensor([truths_stft_1.real, truths_stft_1.imag], dtype=torch.float64)\n",
        "      truths2 = torch.tensor([truths_stft_2.real, truths_stft_2.imag], dtype=torch.float64)\n",
        "      \n",
        "      pred = model(inputs)\n",
        "      \n",
        "      pred1 = pred[0,:2,:,:]\n",
        "      pred2 = pred[0,2:,:,:]\n",
        "      \n",
        "      pred1, pred2, truths1, truths2 = matchAll(pred1, pred2, truths1, truths2)\n",
        "      \n",
        "#       print(truths_stft_1.shape)\n",
        "#       print(truths_stft_2.shape)\n",
        "#       print(pred1.shape)\n",
        "#       print(pred2.shape)\n",
        "      \n",
        "      loss1 = criterion(pred1, truths1)\n",
        "      loss2 = criterion(pred2, truths2)\n",
        "      \n",
        "#       loss3 = criterion(pred1, truths2)\n",
        "#       loss4 = criterion(pred2, truths1)\n",
        "      \n",
        "#       loss_loss = min(loss1+loss2 , loss3+loss4)\n",
        "      loss_loss = loss1 + loss2\n",
        "      optimizer.zero_grad()\n",
        "      loss_loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 10 ==0:\n",
        "      print(loss_loss)\n",
        "    \n",
        "model = KhaledNet3()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmAAYmXMo8va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, train_loader, train_loader, num_epochs = 10000, learning_rate=5e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "04MmvDzgXnyN",
        "colab": {}
      },
      "source": [
        "!mv pytorch-wavenet/ pytorch_wavenet   #renames pytorch-wavenet to pytorch_wavenet cuz dashes/hyphens (the \"-\" ) are forbidden in python\n",
        "!ls  pytorch_wavenet    #show me everythig in this folder ~~ this line is unnecessary and was just for me\n",
        "from pytorch_wavenet import *  #import EVERYTHING"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQZuBXlFtjMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matchSize(a, b):\n",
        "  result = torch.zeros(b.shape)\n",
        "  result[:a.shape[0]] = a\n",
        "  return result\n",
        "\n",
        "def matchAll(a, b, c, d, e):\n",
        "  lens = np.array([a.shape[0], b.shape[0], c.shape[0], d.shape[0], e.shape[0]])\n",
        "  largest_index = lens.argmax()\n",
        "  all_inputs = [a,b,c,d,e]\n",
        "  a = matchSize(a, all_inputs[largest_index])\n",
        "  b = matchSize(b, all_inputs[largest_index])\n",
        "  c = matchSize(c, all_inputs[largest_index])\n",
        "  d = matchSize(d, all_inputs[largest_index])\n",
        "  e = matchSize(e, all_inputs[largest_index])\n",
        "\n",
        "  return a,b,c,d, e\n",
        "\n",
        "class KhaledNet4(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(KhaledNet4, self).__init__()\n",
        "    self.name = \"khalednet4\"\n",
        "    self.conv1 = nn.Conv1d(1, 4, 100, padding = 10) \n",
        "    self.conv2 = nn.Conv1d(4, 2, 100, padding = 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    output = self.conv2(x)\n",
        "    return output\n",
        "\n",
        "  \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:       \n",
        "      batch = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content\n",
        "        truths1 = SPHFile(truth1[i]).content\n",
        "        truths2 = SPHFile(truth2[i]).content\n",
        "        \n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\"))\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\"))\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\"))\n",
        "      \n",
        "      pred = model(t_inputs.unsqueeze(0).unsqueeze(0).float())\n",
        "      \n",
        "      pred1 = pred[0,0,:]\n",
        "      pred2 = pred[0,1,:]\n",
        "\n",
        "      \n",
        "      pred1, pred2, t_truths1, t_truths2, t_inputs = matchAll(pred1, pred2, t_truths1, t_truths2, t_inputs)\n",
        "\n",
        "      loss1 = criterion(pred1, t_truths1)\n",
        "      loss2 = criterion(pred2, t_truths2)\n",
        "      \n",
        "      loss3 = criterion(pred1, t_truths2)\n",
        "      loss4 = criterion(pred2, t_truths1)\n",
        "\n",
        "#       loss1 = torch.norm(pred1 - t_truths1)\n",
        "#       loss2 = torch.norm(pred2 - t_truths2)\n",
        "      \n",
        "#       loss3 = torch.norm(pred1 - t_truths2)\n",
        "#       loss4 = torch.norm(pred2 - t_truths1)\n",
        "      \n",
        "      min_loss = min(loss1+loss2 , loss3+loss4)\n",
        "      recon_loss = criterion(t_inputs, (pred1 + pred2))\n",
        "      \n",
        "      total_loss = min_loss + recon_loss\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 10 ==0:\n",
        "      print(total_loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8nRsYVjf91G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KhaledNetFC(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(KhaledNetFC, self).__init__()\n",
        "    self.name = \"KhaledNetFC\"\n",
        "    self.fc1 = nn.Linear(100, 200) \n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.fc1(x)\n",
        "    return output\n",
        "  \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:       \n",
        "      batch = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content[:100]\n",
        "        truths1 = SPHFile(truth1[i]).content[:100]\n",
        "        truths2 = SPHFile(truth2[i]).content[:100]\n",
        "      \n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\"))\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\"))\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\"))\n",
        "      \n",
        "      pred = model(t_inputs.float())\n",
        "      \n",
        "      pred1 = pred[:100]\n",
        "      pred2 = pred[100:]\n",
        "\n",
        "      \n",
        "#       pred1, pred2, t_truths1, t_truths2, t_inputs = matchAll(pred1, pred2, t_truths1, t_truths2, t_inputs)\n",
        "\n",
        "      loss1 = criterion(pred1.float(), t_truths1.float())\n",
        "      loss2 = criterion(pred2.float(), t_truths2.float())\n",
        "      \n",
        "      loss3 = criterion(pred1.float(), t_truths2.float())\n",
        "      loss4 = criterion(pred2.float(), t_truths1.float())\n",
        "\n",
        "#       loss1 = torch.norm(pred1 - t_truths1)\n",
        "#       loss2 = torch.norm(pred2 - t_truths2)\n",
        "      \n",
        "#       loss3 = torch.norm(pred1 - t_truths2)\n",
        "#       loss4 = torch.norm(pred2 - t_truths1)\n",
        "      \n",
        "      min_loss = min(loss1+loss3 , loss2+loss4)\n",
        "      recon_loss = criterion(t_inputs.float(), (pred1 + pred2).float())\n",
        "      total_loss = min_loss + recon_loss\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 100 ==0:\n",
        "#       print(\"MIN of both: \", min_loss, \"reconstruction: \", recon_loss)\n",
        "      print(loss1+loss2, loss3 + loss4)\n",
        "#       print(total_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn1tSF0T1P9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(m, train_loader, train_loader, num_epochs=10000, learning_rate=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0aQ3AbG4vmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_array = SPHFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/DR5_MPMB0_SX151_DR4_MJPM1_SX311\").content\n",
        "\n",
        "t_input = torch.tensor(input_array.astype(\"int16\"))\n",
        "t_CB = t_input.unsqueeze(0).unsqueeze(0).float()\n",
        "out = m(t_CB)\n",
        "sound1 = out[0,0,:]\n",
        "sound2 = out[0,1,:]\n",
        "\n",
        "sound1numpy = sound1.detach().numpy()\n",
        "sound2numpy = sound2.detach().numpy()\n",
        "\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/test9.wav\", sound1numpy.astype(\"int16\"), 16000)\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/test10.wav\", sound2numpy.astype(\"int16\"), 16000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnFduKFMX5bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To import an SPH file\n",
        "sph = SPHFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/DR5_MPMB0_SX151_DR4_MJPM1_SX311\")\n",
        "\n",
        "#To get the sph file's content (a numpy array)\n",
        "sph_numpy = sph.content\n",
        "\n",
        "#To calculate a numpy array's STFT\n",
        "f, t, Zxx = signal.stft(sph_numpy)\n",
        "#where f is Array of sample frequencies.\n",
        "#where t is Array of segment times.\n",
        "#where Zxx is STFT of x.\n",
        "\n",
        "#To get the numpy array from an STFT\n",
        "t, x = signal.istft(Zxx)\n",
        "#where t is Array of output data times.\n",
        "#where x is iSTFT of Zxx."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drflrE8l791C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is where I tried the concept of using an RNN that takes in a fixed-size array of 46000 numbers (i wanted 48000, which was 3 seconds\n",
        "#(but the file i was working with was a little bit shorted so i went with 46000 just for testing) using a hidden size of 100, then connects\n",
        "#to a fully connected layer which takes the last hidden state and uses it to output 46000 numbers. The output of the FC is a mask that i \n",
        "#multiplied with the original overlay file to generate one of the speakers.\n",
        "\n",
        "#This worked FLAWLESSLY. I had a working proof of concept. I went to swim for an hour and came back to contine in the next big comment\n",
        "\n",
        "class KhaledNetRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetRNN\"\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, 46000)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output\n",
        "\n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:       \n",
        "      batch = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content[:46000]\n",
        "        truths1 = SPHFile(truth1[i]).content[:46000]\n",
        "        truths2 = SPHFile(truth2[i]).content[:46000]\n",
        "      \n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\"))\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\"))\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\"))\n",
        "      \n",
        "      pred = model(t_inputs.float().unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "      loss1 = criterion(pred.float() * t_inputs.float(), t_truths1.float())\n",
        "      \n",
        "#       min_loss = min(loss1+loss3 , loss2+loss4)\n",
        "#       recon_loss = criterion(t_inputs.float(), (pred1 + pred2).float())\n",
        "#       total_loss = min_loss + recon_loss\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss1.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 10 ==0:\n",
        "      print(loss1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSg5QAT7_TqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KhaledNetRNN(46000,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhewuOlK_lat",
        "colab_type": "code",
        "outputId": "ea986549-f8f3-4e87-8f65-719f91f381b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train(model, overfit_loader, overfit_loader, num_epochs=100, learning_rate=5e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([46000])) that is different to the input size (torch.Size([1, 46000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(597388.5000, grad_fn=<MseLossBackward>)\n",
            "tensor(100112.4688, grad_fn=<MseLossBackward>)\n",
            "tensor(49069.6719, grad_fn=<MseLossBackward>)\n",
            "tensor(27902.4609, grad_fn=<MseLossBackward>)\n",
            "tensor(18649.9609, grad_fn=<MseLossBackward>)\n",
            "tensor(13870.1699, grad_fn=<MseLossBackward>)\n",
            "tensor(11171.2051, grad_fn=<MseLossBackward>)\n",
            "tensor(9435.5420, grad_fn=<MseLossBackward>)\n",
            "tensor(8199.4688, grad_fn=<MseLossBackward>)\n",
            "tensor(7267.2676, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HNUkgOHEKU9",
        "colab_type": "code",
        "outputId": "d05ac0a7-0fa5-49b9-c15a-6a8159c542c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for x in train_loader:\n",
        "  inputs = torch.tensor(SPHFile(x[0][0]).content[:46000].astype(\"int16\")).float().unsqueeze(0).unsqueeze(0) #get the overlay file\n",
        "\n",
        "out = model(inputs)  #run it through the model\n",
        "\n",
        "out.shape #debug"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 46000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAsKxJYXE2Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "export_this = inputs.squeeze() * out.squeeze()  #multiply the mask that came out of the model by the overlay file\n",
        "export_this = export_this.detach().numpy()   #ugh idk why i need to do this"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jzYQvoEFryf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/newtest100.wav\", export_this.astype(\"int16\"), 16000) #omgomgomgitsoundsbeautiful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cFjuhCmI6WK",
        "colab_type": "code",
        "outputId": "493cc462-aa43-4ffd-e2c9-a9c31bef5953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.random.randint(-30000,30000, 46000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 25177, -12586,  -2245, ..., -12400,  27598, -26684])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASKuaWksIPhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/noise.wav\", np.random.randint(-30000,30000, 46000).astype(\"int16\"), 16000) #omgomgomgitsoundsbeautiful"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJE8FcLNF3fD",
        "colab_type": "code",
        "outputId": "b798f9fc-20c1-4222-979d-43a8fc46a894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "out #just making sure the numbers make sense (lol idk why i think these make sense but at least theyre not all 1 or 0, right?)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9452,  3.4934, -1.3946,  ..., -0.0664,  0.1471, -0.2035]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz46NIp_GI0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#okayyyy so i finished swimming time to DO THIS\n",
        "\n",
        "#changed the RNN's input size to 32000 (2 seconds worth of audio) jut to make sure I didnt fluke the first one, and to make calculations a little quicker.\n",
        "#again, added a fully connected layer that takes in the RNN's hidden size of 100, but then changed the output to 32000 TIMES 2. so a mask of size 32000\n",
        "#for each of our speakers\n",
        "\n",
        "#again, it worked perfectly. over fit, no noise, super fast (like 2000 epochs/5mins of training to reach a loss of ~5000)\n",
        "\n",
        "class KhaledNetRNN2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetRNN2, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetRNN2\"\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, 16000*2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output\n",
        "\n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:       \n",
        "      batch = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content[:16000]\n",
        "        truths1 = SPHFile(truth1[i]).content[:16000]\n",
        "        truths2 = SPHFile(truth2[i]).content[:16000]\n",
        "      \n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\"))\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\"))\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\"))\n",
        "      \n",
        "      pred = model(t_inputs.float().unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
        "\n",
        "      loss1 = criterion(pred[:16000].float() * t_inputs.float(), t_truths1.float())\n",
        "      loss2 = criterion(pred[16000:].float() * t_inputs.float(), t_truths2.float())\n",
        "\n",
        "      total_loss = loss1 + loss2\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 10 ==0:\n",
        "      print(total_loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz86zPoXgCCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = KhaledNetRNN2(16000,100) #define model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neT-IAC7gIxc",
        "colab_type": "code",
        "outputId": "e400c791-3b6f-47db-fff9-9edc3c8e207e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train(model2, overfit_loader, train_loader, num_epochs=50, learning_rate=7e-3) #train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1106725.8750, grad_fn=<AddBackward0>)\n",
            "tensor(167442.8438, grad_fn=<AddBackward0>)\n",
            "tensor(62490.7383, grad_fn=<AddBackward0>)\n",
            "tensor(27451.7344, grad_fn=<AddBackward0>)\n",
            "tensor(13828.6094, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdCkLUi-gMz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in overfit_loader:\n",
        "  inputs = torch.tensor(SPHFile(x[0][0]).content[:16000].astype(\"int16\")).float().unsqueeze(0).unsqueeze(0)  #get the file\n",
        "  break\n",
        "\n",
        "out = model2(inputs) #get the prediction\n",
        "\n",
        "export_this1 = inputs.squeeze() * out.squeeze()[:16000]  #multiply the first half of the mask by the overlay file\n",
        "export_this2 = inputs.squeeze() * out.squeeze()[16000:]  #multiply the second half of the mask by the overlay file\n",
        "\n",
        "\n",
        "export_this1 = export_this1.detach().numpy() #ugh\n",
        "export_this2 = export_this2.detach().numpy() #not this again\n",
        "\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/Presentation_output1.wav\", export_this1.astype(\"int16\"), 16000)  #output the first dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/Presentation_output2.wav\", export_this2.astype(\"int16\"), 16000)  #output the second dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/Presentation_overlay.wav\", inputs.detach().numpy().astype(\"int16\"), 16000) #output the original file to make sure \n",
        "                                                                                                                        #i wasnt doing anything wrong"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2do-vZ2kPIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#thanks for coming to my tedtalk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOZbYa1HJfPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = torch.tensor([1,1,1,1,1,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAEyvzcxKUZ4",
        "colab_type": "code",
        "outputId": "2b9af916-05ac-4347-a808-dc15cc9a9f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUiCe_lUKXDu",
        "colab_type": "code",
        "outputId": "4df5c9a3-0bc1-4472-d29e-86b7f3c5720a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "math.ceil(len(t)/16000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0t-ypA1KnbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_MZfeTVKzzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunkate(tensor, number_of_chunks):\n",
        "  padded_tensor = torch.zeros(number_of_chunks * 16000)\n",
        "  padded_tensor[:len(tensor)] = tensor\n",
        "  chunks = []\n",
        "  for chunk in range(number_of_chunks):\n",
        "    chunks.append(padded_tensor[chunk * 16000: (chunk+1) *16000])\n",
        "  return chunks\n",
        "\n",
        "class KhaledNetRNN3(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetRNN3, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetRNN3\"\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, input_size*2)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output\n",
        "\n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  \n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:  \n",
        "      batch_loss = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content\n",
        "        truths1 = SPHFile(truth1[i]).content\n",
        "        truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "        t_inputs = torch.tensor(inputs.astype(\"int16\"))\n",
        "        t_truths1 = torch.tensor(truths1.astype(\"int16\"))\n",
        "        t_truths2 = torch.tensor(truths2.astype(\"int16\"))\n",
        "        \n",
        "        number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "        \n",
        "        input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "        truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "        truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "        \n",
        "        for j in range(number_of_chunks):\n",
        "          pred = model(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
        "          loss1 = criterion(pred[:16000].float() * input_chunks[j].float(), truth1_chunks[j].float())\n",
        "          loss2 = criterion(pred[16000:].float() * input_chunks[j].float(), truth2_chunks[j].float())\n",
        "          batch_loss += loss1 + loss2\n",
        "      print(batch_loss)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 10 ==0:\n",
        "      print(batch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2z_tlIEwSGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KhaledNetRNN3(16000,1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGOyLiWLwUH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, overfit_loader, overfit_loader, num_epochs=500, learning_rate=5e-3) #train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3xLT41_xi7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "for x in train_loader:\n",
        "  inputs = torch.tensor(SPHFile(x[0][0]).content.astype(\"int16\"))  #get the file\n",
        "  i+=1\n",
        "  if i == 1:\n",
        "    break\n",
        "\n",
        "number_of_chunks = math.ceil(len(inputs)/16000)\n",
        "input_chunks = chunkate(inputs, number_of_chunks)\n",
        "\n",
        "speaker1 = torch.tensor([])\n",
        "speaker2 = torch.tensor([])\n",
        "\n",
        "for j in range(number_of_chunks):\n",
        "  masks = modelA(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)  \n",
        "  out_chunk1 = input_chunks[j].squeeze() * masks.squeeze()[:16000]  #multiply the first half of the mask by the overlay file\n",
        "  out_chunk2 = input_chunks[j].squeeze() * masks.squeeze()[16000:]  #multiply the second half of the mask by the overlay file\n",
        "  speaker1 = torch.cat((speaker1, out_chunk1.cpu()))\n",
        "  speaker2 = torch.cat((speaker2, out_chunk2.cpu()))\n",
        "\n",
        "\n",
        "export_this1 = speaker1.detach().numpy() #ugh\n",
        "export_this2 = speaker2.detach().numpy() #not this again\n",
        "\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/test110.wav\", export_this1.astype(\"int16\"), 16000)  #output the first dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/test111.wav\", export_this2.astype(\"int16\"), 16000)  #output the second dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/test112.wav\", inputs.detach().numpy().astype(\"int16\"), 16000) #output the original file to make sure \n",
        "                                                                                                                        #i wasnt doing anything wrong"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX051vp19sat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This part is my gpu test\n",
        "\n",
        "def chunkate(tensor, number_of_chunks):\n",
        "  padded_tensor = torch.zeros(number_of_chunks * 16000).cuda()\n",
        "  padded_tensor[:len(tensor)] = tensor\n",
        "  chunks = []\n",
        "  for chunk in range(number_of_chunks):\n",
        "    chunks.append(padded_tensor[chunk * 16000: (chunk+1) *16000].cuda())\n",
        "  return chunks\n",
        "\n",
        "class KhaledNetRNN33(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetRNN33, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetRNN3\"\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True).cuda()\n",
        "    self.fc = nn.Linear(hidden_size, input_size*2).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(1, x.size(0), self.hidden_size).cuda()\n",
        "    x.cuda()\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output\n",
        "\n",
        "  \n",
        "def get_accuracy(model, data_loader):\n",
        "  losses = []\n",
        "  criterion = nn.MSELoss()\n",
        "  for batchData, (truth1, truth2) in data_loader:  \n",
        "    batch_loss = 0    \n",
        "    for i in range(len(batchData)):                \n",
        "      inputs = SPHFile(batchData[i]).content\n",
        "      truths1 = SPHFile(truth1[i]).content\n",
        "      truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\")).cuda()\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\")).cuda()\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\")).cuda()\n",
        "\n",
        "      number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "\n",
        "      input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "      truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "      truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "\n",
        "      for j in range(number_of_chunks):\n",
        "        pred = model(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
        "        loss1 = criterion(pred[:16000].float() * input_chunks[j].float(), truth1_chunks[j].float())\n",
        "        loss2 = criterion(pred[16000:].float() * input_chunks[j].float(), truth2_chunks[j].float())\n",
        "        batch_loss += loss1 + loss2\n",
        "    losses.append(batch_loss.item())\n",
        "  average_loss = np.array(losses).mean()\n",
        "  return average_loss\n",
        "\n",
        "  \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  loss = []\n",
        "  loss_counter = 0\n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:  \n",
        "      batch_loss = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content\n",
        "        truths1 = SPHFile(truth1[i]).content\n",
        "        truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "        t_inputs = torch.tensor(inputs.astype(\"int16\")).cuda()\n",
        "        t_truths1 = torch.tensor(truths1.astype(\"int16\")).cuda()\n",
        "        t_truths2 = torch.tensor(truths2.astype(\"int16\")).cuda()\n",
        "        \n",
        "        number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "        \n",
        "        input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "        truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "        truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "        \n",
        "        for j in range(number_of_chunks):\n",
        "          pred = model(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
        "          loss1 = criterion(pred[:16000].float() * input_chunks[j].float(), truth1_chunks[j].float())\n",
        "          loss2 = criterion(pred[16000:].float() * input_chunks[j].float(), truth2_chunks[j].float())\n",
        "          batch_loss += loss1 + loss2\n",
        "          \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      loss.append(batch_loss)\n",
        "      loss_counter += 1\n",
        "      \n",
        "#       if loss_counter % 10 == 0:\n",
        "#         print(\"Batch \", loss_counter, \": \", batch_loss.item())\n",
        "      if loss_counter % 500 == 0:\n",
        "        plt.plot(loss)\n",
        "        plt.show()\n",
        "      if loss_counter % 1000 == 0:\n",
        "        model_name = \"RNN400\" + str(loss_counter)\n",
        "        torch.save(model.state_dict, \"/content/gdrive/My Drive/APS360 Project/Data/Model Checkpoints/\"+model_name)\n",
        "    if epoch % 10 ==0:\n",
        "      print(\"Epoch Loss: \", batch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-0sgSSKDl54",
        "colab_type": "code",
        "outputId": "19c98115-a861-45b7-d675-24d68ad5a399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "overfit_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/One File/\")\n",
        "overfit_loader = DataLoader(overfit_set, batch_size=1, shuffle=True, num_workers=0)\n",
        "print(len(overfit_set))\n",
        "\n",
        "train_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/Train/\")\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "print(len(train_set))\n",
        "\n",
        "val_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Files Overfit/Validation/\")\n",
        "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "print(len(val_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "20\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWI5KuCW-I2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KhaledNetRNN33(16000,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiIZsz8trmJF",
        "colab_type": "code",
        "outputId": "e16adcc9-6ee0-43ff-91da-36bbace30f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "t1 = torch.randn(1,16000).cuda()\n",
        "t2 = torch.randn(1,16000).cuda()\n",
        "t = torch.stack([t1,t2])\n",
        "model(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4365, -0.4413, -0.2952,  ...,  0.1278, -0.4277,  0.1733],\n",
              "        [-0.1458,  0.6846,  0.1563,  ..., -0.2140,  0.2658,  0.2985]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AeYkXbW_Vnd",
        "colab_type": "code",
        "outputId": "320fa519-8856-4204-98a8-434bb3e45995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(model, train_loader, train_loader, num_epochs=500, learning_rate=1e-3) #train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Loss:  tensor(1637805.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1487712., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1279081.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1206891.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1194151.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1215448.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1210088.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1161341.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1106008.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1079231.5000, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1021484.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(1028594.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(985421.6250, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEDCAYAAAAyZm/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8HMWZ8PHfoxmNbsm6fEqWTzDG\nF1jYHIZgwmFuyMKCcxAWsmSTkLBh3yTwJgsb2M1ustncsIQkvMBuwhWOOAlgCAbsYAOW8W18n5Kx\nJVuWLcuyznr/6K6ZGkljCVu25J7n+/noo5nqnlb1aOap6qerq8UYg1JKqeSR0tcVUEopdWJp4FdK\nqSSjgV8ppZKMBn6llEoyGviVUirJaOBXSqkk028Dv4g8JiLVIrKqB+v+WESW+T/rRaTuRNRRKaVO\nRtJfx/GLyAXAQeBJY8yEj/G6rwJnGGNuO26VU0qpk1i/7fEbY+YDtW6ZiIwWkVdFZImILBCRcV28\ndDbw1AmppFJKnYTCfV2Bj+lR4B+MMRtEZDrwMHCRXSgiZcBIYF4f1U8ppfq9kybwi0g2cC7wnIjY\n4rQOq90M/N4Y03Yi66aUUieTkybw46Wl6owxU46wzs3AV05QfZRS6qTUb3P8HRljDgBbRORGAPFM\ntsv9fH8+sKiPqqiUUieFfhv4ReQpvCB+qohUisjtwGeA20VkObAauNZ5yc3A06a/DlNSSql+ot8O\n51RKKXV89Nsev1JKqeOjX57cLSoqMiNGjOjraiil1EljyZIle4wxxT1Zt18G/hEjRlBRUdHX1VBK\nqZOGiGzr6bqa6lFKqSSjgV8ppZKMBn6llEoyGviVUirJaOBXSqkko4FfKaWSjAZ+pZRKMoEN/I3N\nbTz9/nba23VKCqWUcvXLC7h6w9efWcarq3cxdlAOU8vy+7o6SinVbwS2x//q6l0AhFOkmzWVUiq5\nBDbwW5roUUqpeN2mekTkMeAqoNoYM6GL5d/Amyffbu80oNgYUysiW4F6oA1oNcaU91bFe0qnnVZK\nqXg96fE/DsxKtNAY85/GmCn+LRHvBd42xtQ6q8z0l5+woN/mnNDVsK+UUvG6DfzGmPlAbXfr+WYD\nTx1TjXrB4ZbYvda1w6+UUvF6LccvIpl4RwbPO8UGeE1ElojIHd28/g4RqRCRipqammOqixv4tc+v\nlFLxevPk7tXAOx3SPDOMMWcClwNfEZELEr3YGPOoMabcGFNeXNyjewkkdLi13dnuMW1KKaUCpzcD\n/810SPMYY6r839XAi8C0Xvx7CTU2O6meE/EHlVLqJNIrgV9E8oBPAH9wyrJEJMc+Bi4FVvXG3+uO\n5viVUiqxngznfAq4ECgSkUrgfiAVwBjziL/a9cBrxpgG56WDgBdFxP6d3xljXu29qifW1OoGfo38\nSinl6jbwG2Nm92Cdx/GGfbplm4HJR1uxY3G4xcnx90UFlFKqHwvklbtxOX6N/EopFSeQgf+wm+rR\nPr9SSsUJZuB3Uj1KKaXiBTLwN7oXcGmHXyml4gQy8De16Dh+pZRKJJCBX8fxK6VUYgEN/O5wTo38\nSinlCmTgb3e6+drjV0qpeIEM/CbBY6WUUkEN/MZ9rKFfKaVcwQz86B24lFIqkUAGfs31KKVUYsEM\n/A4d1aOUUvECGfjjOvwa95VSKk4wA78O51RKqYQCGvidx31XDaWU6peCGfjdx9rlV0qpOMEM/Nrj\nV0qphIIZ+NEcv1JKJRLMwB8X7DXyK6WUq9vALyKPiUi1iKxKsPxCEdkvIsv8n/ucZbNEZJ2IbBSR\ne3qz4koppY5OT3r8jwOzullngTFmiv/zAICIhICHgMuB8cBsERl/LJXtKR3OqZRSiXUb+I0x84Ha\no9j2NGCjMWazMaYZeBq49ii287HpjA1KKZVYb+X4zxGR5SLyioic7pcNA3Y461T6ZV0SkTtEpEJE\nKmpqanqpWtrjV0qpjnoj8H8AlBljJgM/B146mo0YYx41xpQbY8qLi4uPqULxwzk18iullOuYA78x\n5oAx5qD/+GUgVUSKgCqg1Fm1xC877nQ4p1JKJXbMgV9EBouI+I+n+dvcCywGxorISBGJADcDc471\n7/WEXsCllFKJhbtbQUSeAi4EikSkErgfSAUwxjwC3AB8SURagUbgZuMNq2kVkTuBuUAIeMwYs/q4\n7EUHOmWDUkol1m3gN8bM7mb5L4BfJFj2MvDy0VXt6GmsV0qpxAJ55S6a41dKqYQCGfh1VI9SSiUW\n/MCvcV8ppeIEM/BrqkcppRIKZuDX4ZxKKZVQMAO/+1i7/EopFSeQgd+lYV8ppeIFMvAbnZ5TKaUS\nCmbgx5AiscdKKaViAhn4MZDiTR+ko3qUUqqDQAZ+A/hxXymlVAfBDPzGILbH38d1UUqp/iaYgR9i\nOX6N/EopFSeYgd/N8WufXyml4gQz8KMnd5VSKpFgBn5joid3Ne4rpVS8YAZ+Yj1+7fIrpVS8QAZ+\nQHv8SimVQDADv17ApZRSCQUy8MdN2aCRXyml4nQb+EXkMRGpFpFVCZZ/RkRWiMhKEVkoIpOdZVv9\n8mUiUtGbFT8SY9ALuJRSKoGe9PgfB2YdYfkW4BPGmInAg8CjHZbPNMZMMcaUH10VPz5vHH/ssVJK\nqZhwdysYY+aLyIgjLF/oPH0XKDn2ah0bL9WjPX6llOpKb+f4bwdecZ4b4DURWSIidxzphSJyh4hU\niEhFTU3NMVUi7spd7fIrpVScbnv8PSUiM/EC/wyneIYxpkpEBgKvi8haY8z8rl5vjHkUP01UXl5+\nTNFaZ+dUSqnEeqXHLyKTgF8D1xpj9tpyY0yV/7saeBGY1ht/rztGh3MqpVRCxxz4RWQ48ALwOWPM\neqc8S0Ry7GPgUqDLkUG9T+/ApZRSiXSb6hGRp4ALgSIRqQTuB1IBjDGPAPcBhcDD/hDKVn8EzyDg\nRb8sDPzOGPPqcdiHTrTHr5RSifVkVM/sbpZ/AfhCF+WbgcmdX3H8GQDN8SulVJcCeeUuxOK+dviV\nUipeIAO/MUZTPUoplUAwAz96By6llEokmIHfONMya9xXSqk4wQz8ODdiUUopFSeYgd8YUlJij5VS\nSsUEMvCDjuNXSqlEAhn4dT5+pZRKLJiBP+4OXH1bF6WU6m+CGfiNDudUSqlEAhz4Y4+VUkrFBDPw\nYxA0x6+UUl0JZOCPo11+pZSKE8jAbwwg3tW7GvaVUipeMAM/3uycgnb4lVKqo0AGfvy5ekRER/Uo\npVQHgQz89uSu9viVUqqzYAb+aI+/r2uilFL9TzADP860zH1aE6WU6n+CGfiNTfWIpnqUUqqDHgV+\nEXlMRKpFZFWC5SIiPxORjSKyQkTOdJZ9XkQ2+D+f762KH0m0xy86ZYNSSnXU0x7/48CsIyy/HBjr\n/9wB/DeAiBQA9wPTgWnA/SKSf7SV7SnbyxfQXI9SSnXQo8BvjJkP1B5hlWuBJ43nXWCAiAwBLgNe\nN8bUGmP2Aa9z5AakV3g9ftELuJRSqgu9leMfBuxwnlf6ZYnKOxGRO0SkQkQqampqjrlCXqZH9A5c\nSinVQb85uWuMedQYU26MKS8uLj7WjQH+lA0a95VSKk5vBf4qoNR5XuKXJSo/ruzJXUFTPUop1VFv\nBf45wC3+6J6zgf3GmI+AucClIpLvn9S91C87rozxUz2iwzmVUqqjcE9WEpGngAuBIhGpxBupkwpg\njHkEeBm4AtgIHAL+zl9WKyIPAov9TT1gjDnSSeJeYTDeyV10OKdSSnXUo8BvjJndzXIDfCXBsseA\nxz5+1Y6e7fGjOX6llOqk35zc7U3RuXr6uiJKKdUPBTPwAyB+jl+7/Eop5Qpm4DcmOjunhn2llIoX\nyMAPegcupZRKJJCB3+gduJRSKqFgBn69A5dSSiUUyMAPaI5fKaUSCGTgj/XydUCnUkp1FMzAj3Pr\nRe3yK6VUnGAGfnvrRb0Ti1JKdRLMwA8gOpxTKaW6EsjAT3R2zvjAv2BDDTN/+BaHW9r6rGpKKdXX\nAhn4o7deJH4c/z+/tIotexrYWdfYd5VTSqk+FszAb0ynHv/63fVs3XsIgNRQIHdbKaV6JJARsKs7\ncF364/nR5eGQDvNUSiWvYAZ+vQOXUkolFMzA79+Byz7uqK1dWwOlVPIKZOCH2Kierobx61GAUiqZ\nBTLw28CeaK6edo38SqkkFtzAL3jDObsI8prpUUolsx4FfhGZJSLrRGSjiNzTxfIfi8gy/2e9iNQ5\ny9qcZXN6s/JHrLM/ZYP2+JVSKl64uxVEJAQ8BFwCVAKLRWSOMWaNXccY83Vn/a8CZzibaDTGTOm9\nKncveutFus7nt2uXXymVxHrS458GbDTGbDbGNANPA9ceYf3ZwFO9Ubmj5Wd6/DtwdaZxXymVzHoS\n+IcBO5znlX5ZJyJSBowE5jnF6SJSISLvish1if6IiNzhr1dRU1PTg2olFr31IiTI8WvkV0olr94+\nuXsz8HtjjDsLWpkxphz4NPATERnd1QuNMY8aY8qNMeXFxcXHVAl760U0x6+UUp30JPBXAaXO8xK/\nrCs30yHNY4yp8n9vBt4iPv9/XLg9/kTLlVIqWfUk8C8GxorISBGJ4AX3TqNzRGQckA8scsryRSTN\nf1wEnAes6fja3ubegct2+VOcVkB7/EqpZNbtqB5jTKuI3AnMBULAY8aY1SLyAFBhjLGNwM3A0yY+\nqX4a8EsRacdrZP7DHQ10vBj/TizeyV2vOuFQCs2t7YBO2aCUSm7dBn4AY8zLwMsdyu7r8Pxfunjd\nQmDiMdTvKHUezhlxAr/GfaVUMgvklbsQm6tnx75DPLFwK5FwbFe7GumjlFLJokc9/pNNdK4ehFVV\nB1hVtZqQk+TXHr9SKpkFsscfvRGLc0LXzevryV2lVDILZuA3/jj+BDTwK6WSWTADP/H32+2ovf2E\nVkcppfqVYAZ+/9aL+xtb4sqLstMA7fErpZJbQAO/d+vF1g5ncVP9m6xr4FdKJbNgBn7/d1uHnI4d\n2aNxXymVzAIZ+PHn6unc4/d2V3v8SqlkFsjA783HL52mZrA9fh3Hr5RKZsEM/P4duDr2+MN+4Ne5\nepRSySyYgR9vVE/HAG9TPTplg1IqmQUy8Fua6lFKqc4CGfjtjVg6CqfocE6llApm4Mcbx2/Z+dnC\nOo5fKaWCFfjnr69hxD1/5nBLe9xMPempIQDCKTbHDw1NrXqSVymVlAIV+J9YuDX2xIn8af5c/LbH\n39ZuOP3+uXzr+RUnsHZKKdU/BCrwu/13d3bOtHB8j/9QcysAv19SecLqppRS/UWgAr/LPbmblur3\n+P1kf90hb/K2rEjohNdLKaX6WnADv/O4Y6qnzp+1MzMtkDcgU0qpI+pR4BeRWSKyTkQ2isg9XSy/\nVURqRGSZ//MFZ9nnRWSD//P53qx8R+6FWW6PP3ZyV3v8SinVbZdXRELAQ8AlQCWwWETmGGPWdFj1\nGWPMnR1eWwDcD5TjpeCX+K/d1yu1P1K943L8XvsW8nP8+xubAciMaI9fKZV8etLjnwZsNMZsNsY0\nA08D1/Zw+5cBrxtjav1g/zow6+iq2r24k7txo3q8nr2dj3+f7fGnaY9fKZV8ehL4hwE7nOeVfllH\nfyMiK0Tk9yJS+jFfi4jcISIVIlJRU1PTg2odWfw4/vgc/75DXo8/S3P8Sqkk1Fsnd/8IjDDGTMLr\n1T/xcTdgjHnUGFNujCkvLi7upWp5Ivbkrk31RHP8GviVUsmnJ4G/Cih1npf4ZVHGmL3GmCb/6a+B\nqT197XHj5HoioQ7DOf1RPV3N56OUUkHXk8C/GBgrIiNFJALcDMxxVxCRIc7Ta4AP/cdzgUtFJF9E\n8oFL/bLjzo3ptscfCsXPx9/WbliyrZbWtvaOL1dKqcDqNtdhjGkVkTvxAnYIeMwYs1pEHgAqjDFz\ngK+JyDVAK1AL3Oq/tlZEHsRrPAAeMMbUHof98Osae+z25u1oHtvjt5Zs28crq3bxtYvGcPelpx6v\naimlVL/SoyS3MeZl4OUOZfc5j+8F7k3w2seAx46hjkfFHc5pGwGb47eq673s1Lrd9SesXkop1dcC\ndeVuouGc9qEdztmRvTOXUkolg8BGPDfE20YglNL17kY08CulkkhgI158j997kqjHH05QrpRSQRTg\nwN/5DlwpCcZvaqpHKZVMAhXxTIJbKtpGQKTrsfsa+JVSySSwEa+rAG9M173+plYdx6+USh7BDfxd\nDOc0QKirwN/SdoJqpZRSfS+wgd9lGwFjTJdHAodbNfArpZJHYGcpE4HPnV3Gjn2H4sq7SvUcbtFU\nj1IqeQS2xy/Ag9dN4PG/mxYd1dNuDCld9fj9VE91/WHa27s+QayUUkER3MAvXT9OdHJ3X0Mz53//\nTV5dvesE1E4ppfpOoAJ/3CRtcSd3JbY8QY+/qq6RptZ2du0/fJxrqZRSfStQgd/V1Vw97YYuUzmH\nW9qo8Sds06GdSqmgC1Tgb09wARfR4ZyGli4Df7sT+HWEj1Iq2AIV+FudoO5O2RAbzgkt/k1X3JO8\nTa1t1BzUHr9SKjkEKvC3uYHfKXfTPvagwL3frtvjb2xu4+WVH+noHqVUYAUq8Mf3+GPl9qE7l09a\naij6uLGljep676Tu4wu38uXffsDzH1Qe17oqpVRfCVTgb++mx++eArC3YcxJC9PWbqiqix/Ns6M2\n/sIvpZQKikAF/tYE6Zlojt8pC/mBPz8rAsC2vQ1xr7E5f6WUCppABf629tiJ2biTu130+HPSvRx/\nXkYqAHWHWuK2tX73weNUS6WU6ls9CvwiMktE1onIRhG5p4vld4vIGhFZISJviEiZs6xNRJb5P3N6\ns/IdtXWX43f6/Ll+wLcNQEf2ZK9SKliW7ajjYFMrh1va2NvhyL6t3dDsj+zbE+Cj/m4naROREPAQ\ncAlQCSwWkTnGmDXOakuBcmPMIRH5EvAD4CZ/WaMxZkov17tLiUb14F6567M9ffu7o0PNrWysPkhm\nJMTQARm9XVWl1AlUf7iFdbvqmVQygOseeoczhw+gICvCXz6sZtV3L+O6h97he9dP5D9e+ZAVlfv5\nn9unM/tX7/LrW8opzI6Qkx5mzMCcvt6NXtOTHv80YKMxZrMxphl4GrjWXcEY86Yxxp4NfRco6d1q\n9kxcjt/p8mdGvBE8aamx3U33R/Xkpqd29RIONrVy8Y/e5oIfvMmSbfu45EdvU384Ph2klDo53PvC\nSm54ZFF0tt4PttfxxtpqAF5fs4uN1Qf5P88t54PtdbS2G7b65/yerdjB9Q8v5OIfzaeptY2l2/f1\n2T70pp4E/mHADud5pV+WyO3AK87zdBGpEJF3ReS6RC8SkTv89Spqamp6UK3OEvX4/+68Edz1ybHc\ndt5IZ13vcM5N9WT7Y/sHZKZGp2pubTf8cO46NlQfZPHW2qOql1Lq6LW3GxqbvSvqn1m8nbpDzSzY\nUMMzi7ezv7GFO56s4KP9jby6ahfz1u7mcIt3LY4xhr99ZBE/eHUtO+saAVhVtT+63QlD8wD4YFsd\nANudkXxb9niBf/XOA9GyH722nusfXsi7m/fyszc2sGjTXlZW7ucHr65NeNvX/qpX5+MXkc8C5cAn\nnOIyY0yViIwC5onISmPMpo6vNcY8CjwKUF5eflTvYqIcf1o4xNcvOSVu3dY2b91cJ9WTnR6mvqmV\noXkZcSd7i3LSANhR28jXnlrKrAmDuWLikKOpolKqh9btqqckP4P/em09j72zhQXfnMm3nl/J797b\nzvJKL4BnRsK8tmY3re2GeX4P/luzxvH9V9fy89ln8P7WWt7fWsunzhzGB9vrWFEZC/w2E7BqZ6ys\nICtCbUMzy7Z7jUGV32AALNq8F4A/LNvJU+9vB2DCsFxWVR3g6slDeXT+Zm45p4wzhucfx3eld/Sk\nx18FlDrPS/yyOCJyMfBt4BpjTPSsiDGmyv+9GXgLOOMY6ntE8T3+LqbhdNi0UK7f4w+lCBn+B6Fj\nTt/2NpZu38ec5Tv5y5rdNLe2R0/+zFu7W9NASh2Dt9ZVs3rnfpbtqONzv3mPhqZWLvvJfK596B1e\nXvkRAH/duAcgGvQBNlR7o+/cFMxH+71g/fb6WObApnaX76iLlu064F274w7kSA15ceOjA7GAbzX5\nWYDVcQ2F1yl8tmIHLy6t4vqHFzJv7W5+9sYGahuamfrg6/x1wx7u/8MqPv2rdznU3Mr/vrutz+cE\n60mPfzEwVkRG4gX8m4FPuyuIyBnAL4FZxphqpzwfOGSMaRKRIuA8vBO/x0WiK3ddv/vCdLLSwvzX\n6+uBWI+/MCsSPZs/dEB63GtWVHofloWbvBa/qq6Rbz2/gheXVvHOPRdx2+MVXHzaIO65fBzV9Yc5\nd3QR723ey1kjCkjp6s4vSiWJ/YdaOHC4hWEDMnh68Q6unTKUxVtraWptZ3RxNjf9chHP/sM53Pr/\nFgNw7ZShLNiwh2crvOzyxuqDXD5hMK+s2sXra3ZHt1tWmMm2vYdYtMlrDPY5R+g2PWO/twAr/cbC\nTedUH/ACvhv49ze2xC1z2av73WyAHRXkNii3PV4BeGnkvQ3NfPm3SzhwuBWAh9/cxC/e3MgH2/bR\n2NJGZiTMD2+cFDf8/EToNvAbY1pF5E5gLhACHjPGrBaRB4AKY8wc4D+BbOA5fwe2G2OuAU4Dfiki\n7XhHF//RYTRQr0o4qsdx7pgiAIYXeL36kvxMAIpz0tjt9wA69vir/Q+G/V1V18h7W7x8/wfbvJ7G\nyqo6Lv7R2wA8eds0bnnsfb5z5WncWF6KSPxJZPXxePdK1ga0PzHGcLCplZz0VJ5ctJWJw/LYe7CZ\nh97ayP/ePp3zvj+Puy85hT8u38nirft4/kvn8H9fXMlb66p5zQ/g35o1jr0NzfzY74QBpIW9JMRc\n54ZIlfu83veG6vpomT0KdwO5iDdyz151v9sJ3nY998LMRv/Oe+7EjPbcXleTNdrGxTYAEEsFuSmh\ngTlpVNc38cpKbx8OHG4lLyOV/Y0tvOuni15aVoUNV6cPzeWlZVXcd9V4ykcUdPq7x0OPcvzGmJeB\nlzuU3ec8vjjB6xYCE4+lgh9Hohx/V75z5Xg+OW4QZ43IR8QL/Nv2eh+O7oZvujdrsf9Ie84AYP1u\n7wO6YfdBpjzwGikiLLr3In44dx3fvWYC1fWHyUlPZUBGKvVNrQmHlCarj/Y38vVnlnHLOSN44YNK\n3l5fw5++ej5/WrGTkUVZfOrMEppb24mEU2hvN4hAS5thf2MLxTlp2lD0EmNMdAj0u1v2Mn1kIU8s\n3EpOepiMSIg7f7eUOXeex31/WA3AtBEFLN1exxOLtlJ3qIX7/rCaSMgL5DbYv+b02pf4naZF/pE0\neKNtIHZyFWJX1X/kTKtiO2FucLd1tcts79193BvnYN17dNvev1sP+7fcRinVfx9smTsA8dmKHazd\nVc/3X13Lc/9w7rFXsAcCdbP1+68ezz0vrAS6z/Gnp4aYOW4gAJmpIYqz05g2soB5a6sZmpd+xNe6\nKSUb+O10z+AdngKkpAjGQJsx/Oer63huSSXnjC7k688spzgnjdnThvOzNzaw/L5LufOpD/js2WWM\nH5LL+t31fPK0QazbVc/o4iwOt7bT0toenV6iv6s/3IKI9x94fc1urp0ylFdX7aKsMIvSggxWVu7n\n3DFFrNl5gOGFmXz40QFqG5pZtGkv//vuNmZPG867m2t5d3NsFNX3X13LvLXVDMxJY8GGPfxpxU5+\neONk/vGZZXztorE8uWgrDU1t/O7vp/P3T1bw4HUTyM+MsGVPA589u4ztew9Rkp+hqbcONlbXU5Kf\nyYbdB1m6Yx9/W17KXU8v5UsXjuGlpVXMXb2L731qIn/3/xYze1opT73vpWBmT/NO+/3kLxui27IX\nSP55xUfRspFFWazbXc+ba6MZYIqyI+w52MzKKi/I721oji7bXON9d9xAatMkiaZk6W/s0YI9hwCw\ntyE+a+Da5O/znoPNnZYdL4EK/Df7gXTnx7x94lc/OZYzh+czYVgulfsao3P1p0isZR5ZlBXXC7E2\n1Xhl9sMJ8L6fBjrg9Djq/eUHm7zDy5r6Jp5Z7I0MWLpjHws27GHBhj3kpHkjixZ8cyaX/3Q+P7xx\nMvPX17B17yF+9LeTufYX7/DCl89lzvKd5GWkcvnEITz85kbuu3o8c5btZNzgXAqyI/x1Qw03nTWc\nOct3cvbIAhpb2qjc18i5owv588qPuPi0QdTUN9HS1s7IoixWVR1gYkkeO+saSU8NkZeRysbqg5w6\nOIfVO/czzD8K2lRzkKllBby3eS8ThuWxuaaB3QcOM25IDnc/s5wHrjudWT9ZwNSyfFrb2lleuZ+a\n+ib+7eUPGTc4h7pDLew6cJgf3DCJb/5+BddNGcpLy3YCMGZgNq3thrfWx4KEVeEPpW1rN7y41Btb\n8NM3NmAM/GFZVfQw/NsvrmLfoRaerahkvn9yr7WtnX/54xp+8ekzeOPDarLSQvzNmSU8/NYm/vW6\nCfzzS6s4syyfGWOKeHzhVr53/URW7dxPWUEmhdlptLebo2owjDFsrz1EWWEWf1y+k8F56RRmRXhy\n0Ta+c+VpfPP5FVw5cQiD89JZtGkvXzh/FG3tJjqP1NHYUXuIouw0NlTXs+dgE1NK8/nvtzZyyzkj\neHnlR6SIcPrQXH42bwMPXDuBS388n6LsNEYXZ/Hellpq6puYu3o3c1fHeua/WbAFIBr0Ad7zG+UP\nnJOqdsTM2l2xlIxNz7hToNgAt7uLPPpJEts/tiMdabT42YKddY0n7Gg1UIEfnDl6PsZ79w+fGB19\nfMqgnOiogIzUEA1+LnFSSR5b9jRQlJ3W7aXcm/0GYplzwsdu054TgFh66B1/tEIklEJ9k9dAvLW+\nhnbjHfIu21HHnoPNLN5aS31TK6t27ud3722npCCTt9fXsGDDHi4ZP4hv/H4FAMMLMtlee4gJw/L4\n2lNLOW9MIe9s9I5MfvP5cu783VK+etEYfj5vIwD/et0EvvPSKv7n9ml87jfvU5KfwUXjBvLkom28\n9vULuPJnf2XGmCJ21jWyeU8DL3z5XG569F1uPXcEjy/cCsD0kQW8v7WWb/p1WOLsp11nz8Hm6Hv3\nC/9vz3N6gvaQfkdt5xEVtmEoFjGTAAAUJElEQVSN7x1662/dGzukXuen2d510gfff3Wd/3ttdNvP\nLN5BS5uhpr6JZTvq4lIQo4qz+MGr6zh/bBFnlA7g+Q+q+O0XpvPzeRu54JQiBuWms3rnAf7mzGE8\nvnArM8YUsedgE+9s3Mtnzh7ObxZs4dTBOWzZ08Bv39vOT2+ewl1PLyM7LcxB//9bkp/BCx9UsWDD\nnujJxYKsCP/03HL+7bqJ7DpwmKLsCEPzMqg52ERTSxs79jVyoLGFt9fXcMXEIbyzcQ/pqSFmjC3i\npaVVXDN5KL+cv5n01BTCKSnRvwXwKz94AwwbkEFVXSOf/fV7/v+lKfp/+eX8zdH1siLe53+hfwLV\nZT/j7olO29N1U65BDeTHQ1NrOzUHmxiYc+SMQ28IXOAflJtGVV0j+w8d/fDKDH/o17ghudEANqlk\nAH9YtpPpIwv4sz+8zB4R5KSHoz16l3vCxw5Be29zLCDZ1yzY4Af+cArNfsroLT8gbq5pYFvtIYyJ\nBdOVlQfY29BMKEWoT/P+hYuc7do8oj3k3lMfC5Z2ZNJHzlGRPZH21jqvh1y5r5EnF20D4Fd+IFi6\nfV+0EbRB2w6zg1jAdcdJd3wf3AbT1tE9Umpp670o0eyk3uxJPLdBsX/LbZytH/gNhT0KA7jwh28B\nxN2n4cE/eeMU3HTH/7y7rdP27np6GUBcIP7XP38IxI8oufvZ5QD83xdXHnnniDWmACv9i5Js0PZy\n0InvJGf/H12lHZqdk5r2/x2U4B1KkbhGqT+q3Nd4QgJ/oGbnBLj/6tMBGDfk6OfVGJAZ4ZHPnsmv\nbimPlo0dmO39HpQdLbtk/CAAzh1dGC0r9i/2SsRNQ9ngZA+N3cDw5jov8M9fXxM9TLSB2aZCag42\nRUcizV0VGwVh/ckP/O5JLtsY/HVDrBdnr2bs6uYzzy3xymwQgFgv3Q0cHWc3VSePY0kt9SY7hj4r\nEuq0zC0bkJka9zs3PRxNzxb458Fy0sKd1neHaRf66+V2MUmjXeYOurDrZTvbtd/1ouzYd95dnoj7\nN209bf13fcw09dEKXOCfXDqADf92OeePLT6m7cyaMISCrAj/e/t0vnPladEPlB3+CURb5onD8qJl\n9kNz8WmDjunv245JvdMY2EBrUxzGxAKym+6wbK/aPclkH7tlNj/eH4K3zdS5X1z7nhY4J7ftOYdh\nzgisssLMuLJQikSvzrQn7N30qV2vtCCjU9kQ5wS/fe2g3NgX3AYT90vfcX23EzCyKAvw0kjW6OLO\nZUXZ3j4O7KIDYevk1tf+Lbe+g3O9x2MGZtNRSX7nfR7l1214Qeyzndth2nKXraO7f3a77v/D1qmr\nwRL2b9n30S0rdephy9zvnf0b9m9mRsLRC6mi9ciP1aOrug326zTYqZv9bAzxGwjbEEFspJ/beEQ/\ng138Lbvv6akp0Wlh7PtQ6HxmhkX/H97+1TacmBO8gQv8EBs61RtmjC3iC+ePYvyQXP75qvHMmjCY\n+64azzdnnRr90Lgf1LD/Ybn+jM7TGd1yjjdb9SnOUcO4wd6Rif0yufpLT6ynbBB0v+g2+Ix1gpB9\nfNqQ3GiZvSPamf7l7u6EevYS+DHFsW1MKR3g/R7u/Rbx0nHe+t7vgqxItAdmt+GeZLPr5WfG3vsz\ny7z13MZ8gv94sr99gGJ/X6eUxtbruL7b+zvNPwJ192G0/9h+Brz3IcVf33tv3M/Aqf56boA+xS87\n1dmGDdY2oLtOGeSt515XMqqLBsgGOvez2rHebqMwyi8bURSrm/1ejHS2a98T20gfbokdSdr9KnS+\nC7HGwKtPakiiDVup3xhkRELR748NvIXZkWjKtmSAt97QvFiAto2W26DY98Su7x6Bl0Ybqljd7N8q\n6SLw2/UzUkPRv+WWdVzfvpca+PuZlBTh9hkjyU4Lc9uMkXz5wjH8/fmj+M8bJnH1pKF8a9Y4Hvr0\nmfzwxslcf8YwLj19EFdMHMwjn53K3ZecQnZamFvPHQHAhacOjG7XpotmOmU2WF49qfN8QDYgFDq9\nX/slcgOu/SIM6aK3Zdd3e9WnD82N2z7AVD8ITnMuKrHB4YJTOh9RXXCKd3FcyOkp2TSYDTju35o+\nMrZd28O1Ze7QtvIRXj0OONNi2MB/ysAcMiMhctNToz1o23gUZkXIigb+WNC27Hpu8Jk4zKubG9Rs\nIJrgNAa2/XAbA2tSibeeG7Tt/rvbtVOEuO9Nm98y2VSl2/af6q8XcTo2NnC4DYodgTS6ix6//Wy5\nue4RfgMxyMkt2yvaxw7qnDK123XfN9vIuI3oID9AjyiMBf6stFBcmTsmvswvc88zDC+M7/Gnh0PR\n/4fd97RwSrTTYdfLSA1RlOOtZ3vVeZmpZEZCiMSOHuOOcjLCcdt16xYri+2z3a793oVSJLrPpW49\nsuMDf2bEDfxeWVu7ISc9rIH/ZBAJp3BjeSkpKcKXLhzNlZOGMG5wLj++aQqpoRQe/sxUZk0YzNc+\nOZZV372MUcXZvHLX+XzjslP589dm8MRt07jprFLKCjP54idGM2xABlNKB3BTuTdG+rYZ3myiYwZm\nR7+wd/uTzbmHl5+ZPhyAqU6Atj3c65wjD3tY/elp3vpuGunGqd5M2u4sgzP8q5zd3qT9oF7oBH7b\ng/mEX+YO07ONR11j7AM92O95uRPkDfS/MJNLOwdSuw335PDpfoDOz0qlrDCL/MzUaDAZWeQ9L8iK\nRL9kboNm2QDd6HyZbWB2x4zbG7vlO2mJdv99mljSucd/uj/rY9iJ2na7kXDsK2dHdbkpCHvf6PF+\nfd2hffYz4J6eTAt7+1fg9JJtuzC6OHHgd687sSnLsNNg2/fNPVJL94/CbCNzyDnvY1NZ7nZtisP9\nrGZ16PG7bBBuaOp8FDAwN41IKIX0SKhzkI/EGgP7XqalhijskP5JT/XWSwunRP8fbvowJy01bv2u\n6nbQGYxg/352WioDMlPJTA1F0zh2/YxIKHp0aLeb4QR+W9+m1vboBHEnQuBG9fR3NgDZ4ADw9jdm\nAvDWNy5E8HoOF48fxMiiLF79x/MpzPKuRn17fQ0XnzaQb1x2KpeMH0RVXSNLtu7j5mnD+evGvXxl\n5mgG56bR0NzG7LOG8+baam6cWsJrq3cxpTSf88YUcvezy/nUmSX8+ytruX3GSFLEG+p39eSh/Msf\n13DD1BJeX7Ob97bU8olTi/npGxs4Y/gAnl68nXBKCtefMZT562s4y29kppQOYPqoAn759mbOHe01\nFFdNHMKehmbmr69hmt+Dn3nqQDJSQ3z4UT1XTRrCI29v4vyxRfx83gaGDchg9rRSlu2o49RBOWSk\nhrjprFLOLMvnv15bx6SSPE4bksvXLx7L5j0N/GXNbk4fkseIwkwmDstjz2lN7G1o5uxRhUwbWcCE\nYXncMLWEoQMyKCvM5PYnKpgwNI+xA7O56axShuRl8MSirUwsyWNSSR7fvGwcexuaWLPzAJeOH8wT\nC7fxlZmj2ba3gdZ2w6WnD+KZih1MLfP2ZXLpAM7293m8f/Ry5cQhHG5p44211dEgeMbwAdET9zbF\nYXuEEAuubi/Z9vjtUUBqimBDgU0zuOkqe1ThlqX4jUXHOacgdl7B7fHbgO6WdVW3zEiYwy3N0R5/\ng9NxsEeWbmPQ1YlOW+amRy0bLA81x7ZrA2OmH7RTQynR4Do4L50U8Y8COgR5t6ftHi0UZKVRf9ib\nagK8azws21C5ef9witDabqI9eHcUWskAG8hTKMyKcOBwa/RIe1h+BiLxaSi7jfgevw38bRRlp2ng\nT0buuQkbPMYNjvVUb/SPBL4ycwzgBQebInrytmkAfPvK8dH1N37vCgDe+KcLo2XXnzEMEWHT966I\nphG+cdk4IuEU1j44i7RwCrecM4KWtnay0sIsvOcihuSlc/H4QRjj9VzPG13EwNx0lt93KZFwCmnh\nFG47byTFOWlUfOdictNTaTeGA40t0fVy0sN84fxR0Xps+fcrEBFWf/cyUkRITw1x+cQh5Kan8uGD\ns6LrXTN5KACv3HV+tMxed/GW32C60+A++8VzOr0PW/79SgBevzs2W/iVfhptzp0zomXXTvGOjl72\n/9YLXz4vumztg7NITw2x+ruXEQ4JIRFuOWcEA3PSWfzti8nL8Pf5cAsDc9J59ovnMKkkj89ML2Pf\noWbOGVVIQ1Mrs6cP55zRhURCKZQVZlKSn8nMcQP54gWjOGP4ALLTUnnk7U2MHZjNZ6YP54apJazd\nVc+8tdVM9o8ubj13BCOLsqg52MTnzi7jL2t286kzhzF39S7GDMzmlEE5rKjcHz0CmjaygFFFWTy9\neEf0KOCayUN5/oMqquoao0HbPfIYPySXl1fuil4tnpMWZszAbN7fUkuZH6Cz0sI0tXqByjZoWZFY\nSLGP3QbClqU7ee7UkNDSZijx8/juCDKbH0/3A7kXIL06ZaeFKciKkJ6aEk3ruKmecLb4ZfYoIIWi\nrAi79jdGT167gdwegbqNaG5GKrUNzdETvu4svANzY3UrzE6jpc1EG6Cc9DD5mZEuUz0ZqaHovEL2\naKippZ3CrAhVdSdmVI8G/iRjUwdu/jkS9h7bL2MkLNGUhD3J554MtGmZPCf1Yb/47igXuz13vY71\nyHQCRX+eyM7uS5bTi7WB0h3dYtezRzrueYFb/RsBneWk5O66eCwA915xWrRsxljvyOnfrvemuTpj\neD6z/fTc1v/wGjE73QjAvP9zIRBrxIwx3Di1hMLsNBbdexEDMiJEwil856rxZKeFYw3xBaNoaGpl\ncG46j35uKjPHDeSCU4oxeCe3JwzL44KxRfzl7gvITU8llCLMW1vNiKIsvnvN6Zw7upCUFGFV1X4m\nleTxL1eP5+rJQ7lmylB27T8cnYnz8+eMIDMSZkBmKuMG5/LF/1nClJIBjBucw1WThjC6OJufvrEh\n2lDdeu4I/rh8J2t31TO8IJNQijAwN41TBuVQd6iZcYNziYRSKMnP4JRBOZQWZHLa4FyyIiGGDEhn\nVFEWZYWZtLQZ8jJSGV6QSU5a2JuyQwpITw35o/4+ZOa4gTxbsYP01BCzTh/M62t2R881XHBKMVdM\nGMw9L6yMdsRuPXcEtQ3NPLekktKCTIpz0hhTnM3o4mxSJHaUVZAVoTg7jcxIOHo/jwGZqeRlpJKd\nHubS8YOYu3p3tKEqH5FPViTMqqrYjV+OJ+mPd44pLy83FRUVfV0NpVQfaWhq5WBTK4Ny09lR682z\n1NJmMBjSwqHoNBpNrW2ERAj5KZnUUAqtbe2EUoSm1nb2HWpmSF4Gh5pb/Z52rMNjp0c43NKGiHe+\nZF9DM/lZEVra2gmJxE3V0dZuokfJxhC37HBLG+3GkJEaYsm2fUwty2fhpr1kREIML8jk1wu28I3L\nTmXRpr2UFmQwKDedyn2NjBmYzYcfHaCsMJMl2/axbe8hPnt22VG9ZyKyxBhT3v2aGviVUioQPk7g\n11E9SimVZDTwK6VUktHAr5RSSUYDv1JKJRkN/EoplWQ08CulVJLRwK+UUklGA79SSiWZfnkBl4jU\nAJ3vYdczRUDnm4QGm+5zctB9Tg5Hu89lxpge3YGqXwb+YyEiFT29ei0odJ+Tg+5zcjgR+6ypHqWU\nSjIa+JVSKskEMfA/2tcV6AO6z8lB9zk5HPd9DlyOXyml1JEFscevlFLqCDTwK6VUkglM4BeRWSKy\nTkQ2isg9fV2f3iIij4lItYiscsoKROR1Edng/873y0VEfua/BytE5My+q/nRE5FSEXlTRNaIyGoR\nucsvD+x+i0i6iLwvIsv9ff6uXz5SRN7z9+0ZEYn45Wn+843+8hF9Wf9jISIhEVkqIn/ynwd6n0Vk\nq4isFJFlIlLhl53Qz3YgAr+IhICHgMuB8cBsERl/5FedNB4HZnUouwd4wxgzFnjDfw7e/o/1f+4A\n/vsE1bG3tQL/ZIwZD5wNfMX/fwZ5v5uAi4wxk4EpwCwRORv4PvBjY8wYYB9wu7/+7cA+v/zH/non\nq7uAD53nybDPM40xU5zx+if2s22MOel/gHOAuc7ze4F7+7pevbh/I4BVzvN1wBD/8RBgnf/4l8Ds\nrtY7mX+APwCXJMt+A5nAB8B0vCs4w3559HMOzAXO8R+H/fWkr+t+FPtaghfoLgL+BEgS7PNWoKhD\n2Qn9bAeixw8MA3Y4zyv9sqAaZIz5yH+8CxjkPw7c++Afzp8BvEfA99tPeSwDqoHXgU1AnTGm1V/F\n3a/oPvvL9wOFJ7bGveInwDeBdv95IcHfZwO8JiJLROQOv+yEfrbDx7oB1beMMUZEAjkmV0SygeeB\nfzTGHBCR6LIg7rcxpg2YIiIDgBeBcX1cpeNKRK4Cqo0xS0Tkwr6uzwk0wxhTJSIDgddFZK278ER8\ntoPS468CSp3nJX5ZUO0WkSEA/u9qvzww74OIpOIF/d8aY17wiwO/3wDGmDrgTbw0xwARsR00d7+i\n++wvzwP2nuCqHqvzgGtEZCvwNF6656cEe58xxlT5v6vxGvhpnODPdlAC/2JgrD8aIALcDMzp4zod\nT3OAz/uPP4+XA7flt/gjAc4G9juHjycN8br2vwE+NMb8yFkU2P0WkWK/p4+IZOCd0/gQrwG4wV+t\n4z7b9+IGYJ7xk8AnC2PMvcaYEmPMCLzv7DxjzGcI8D6LSJaI5NjHwKXAKk70Z7uvT3T04gmTK4D1\neHnRb/d1fXpxv54CPgJa8PJ7t+PlNd8ANgB/AQr8dQVvdNMmYCVQ3tf1P8p9noGXB10BLPN/rgjy\nfgOTgKX+Pq8C7vPLRwHvAxuB54A0vzzdf77RXz6qr/fhGPf/QuBPQd9nf9+W+z+rbaw60Z9tnbJB\nKaWSTFBSPUoppXpIA79SSiUZDfxKKZVkNPArpVSS0cCvlFJJRgO/UkolGQ38SimVZP4/Dr862yn9\nlBAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Loss:  tensor(991427.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(978408.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(942203.1875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(932931.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(944169.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(881654.8750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(875671.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(898055.8125, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(862312., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(859057.3125, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(900063.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch Loss:  tensor(862082.7500, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEDCAYAAAAyZm/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8XOV97/HPTzNabFnyJtl4twEn\nmCUsUQ00pGQBYpM2bl437TVJWtrSukmTLmlft4WbXrglIbfbq2nTLMQ38U2bNJCEQuIGB5JAwhIC\nsQzG2GBjecGWsS3Zsq1dmuV3/5gjMZaPRiNptJ35vl+veXnOc86ZeY6P/Z1nnvOcZ8zdERGR4lEy\n0RUQEZHxpeAXESkyCn4RkSKj4BcRKTIKfhGRIqPgFxEpMpM2+M1sk5k1mdnOPLb9rJltDx6vmtnp\n8aijiMhUZJN1HL+Z/QrQDvy7u186jP3+GLjS3X9vzConIjKFTdoWv7s/CbRkl5nZBWb2iJltM7On\nzOyikF1vAe4bl0qKiExB8YmuwDBtBD7i7nvN7Grgi8C7+laa2TJgBfD4BNVPRGTSmzLBb2YzgF8G\nvmNmfcXlAzZbDzzg7qnxrJuIyFQyZYKfTLfUaXe/Isc264GPjVN9RESmpEnbxz+Qu7cCB8zsNwAs\n4/K+9UF//2zg5xNURRGRKWHSBr+Z3UcmxN9sZo1mdhvwIeA2M3sR2AWsy9plPXC/T9ZhSiIik8Sk\nHc4pIiJjY9K2+EVEZGxMyou7NTU1vnz58omuhojIlLFt27YT7l6bz7aTMviXL19OfX39RFdDRGTK\nMLPX8t1WXT0iIkVGwS8iUmQU/CIiRUbBLyJSZBT8IiJFRsEvIlJkFPwiIkUmcsHv7jz4fCNt3YmJ\nroqIyKQUueD/6Z5m/vzbL/KvjzdMdFVERCalyAX/ttdOAVAej9yhiYgUROTSsTXo4plbWTbBNRER\nmZyGnKvHzDYBvwo0ufulIev/B5l58vtebxVQ6+4tZnYQaANSQNLd6wpV8cEk05lppmOxyH2miYgU\nRD7p+DVgzWAr3f0f3P2K4CcR7wCecPeWrE3eGawf89AHSKUywR8vsSG2FBEpTkMGv7s/CbQMtV3g\nFuC+UdVolPpb/KbgFxEJU7D+EDObTuabwX9mFTvwQzPbZmYbhth/g5nVm1l9c3PziOuRSqdHvK+I\nSDEoZEf4rwE/G9DNc527XwWsBT5mZr8y2M7uvtHd69y9rrY2r98SCJVI66ckRURyKWTwr2dAN4+7\nHwn+bAIeAlYX8P1C9fXxO/oAEBEJU5DgN7OZwPXA97LKKs2squ85cBOwsxDvl0tSLX4RkZzyGc55\nH/AOoMbMGoG7gFIAd7832Oz9wA/dvSNr1/nAQ5a5yBoHvunujxSu6uH6+vhd+S8iEmrI4Hf3W/LY\n5mtkhn1ml+0HLh9pxUZKLX4Rkdwid5dTsr+PX0REwkQu+FNq8YuI5BS54E+qj19EJKfIBb9a/CIi\nuUUu+BMaxy8iklPkgl8tfhGR3CIX/EnN1SMiklPkgr+PLu6KiISLbPCLiEi4yAa/GvwiIuEiG/wi\nIhIuusGvTn4RkVDRDX4REQkV2eBXe19EJFzkgt/0I+siIjlFLvj7qItfRCRcZINfRETCRTb4XU1+\nEZFQkQ1+EREJF9ngV3tfRCTckMFvZpvMrMnMdg6y/h1mdsbMtgePO7PWrTGzPWbWYGa3F7LiIiIy\nMvm0+L8GrBlim6fc/YrgcTeAmcWALwBrgYuBW8zs4tFUdjjUxS8iEm7I4Hf3J4GWEbz2aqDB3fe7\ney9wP7BuBK8jIiIFVKg+/mvN7EUz+4GZXRKULQIOZ23TGJSFMrMNZlZvZvXNzc2jrpAa/CIi4QoR\n/M8Dy9z9cuBfge+O5EXcfaO717l7XW1tbQGqJSIiYUYd/O7e6u7twfMtQKmZ1QBHgCVZmy4OysaF\nxvGLiIQbdfCb2XkWTJBjZquD1zwJbAVWmtkKMysD1gObR/t+IiIyOvGhNjCz+4B3ADVm1gjcBZQC\nuPu9wAeAj5pZEugC1numuZ00s48DjwIxYJO77xqToxARkbwNGfzufssQ6z8PfH6QdVuALSOrmoiI\njIXI3rkrIiLhIhv8urYrIhIussEvIiLhIhv8rlu4RERCRTb4RUQkXGSDX338IiLhIhv8IiISLrLB\nrwa/iEi4yAa/iIiEi2zwq49fRCRcZINfRETCRTb4NY5fRCRcZINfRETCRTb41ccvIhIussEvIiLh\nFPwiIkVGwS8iUmQiG/z6sXURkXCRDX4REQk3ZPCb2SYzazKznYOs/5CZ7TCzl8zsGTO7PGvdwaB8\nu5nVF7LiQ1GDX0QkXD4t/q8Ba3KsPwBc7+6XAZ8CNg5Y/053v8Ld60ZWRRERKaT4UBu4+5NmtjzH\n+meyFp8FFo++WiIiMlYK3cd/G/CDrGUHfmhm28xsQ4HfKyf19IiIhBuyxZ8vM3snmeC/Lqv4Onc/\nYmbzgB+Z2W53f3KQ/TcAGwCWLl1aqGqJiMgABWnxm9lbgK8A69z9ZF+5ux8J/mwCHgJWD/Ya7r7R\n3evcva62tnbUddLFXRGRcKMOfjNbCjwI/Ja7v5pVXmlmVX3PgZuA0JFBIiIyfobs6jGz+4B3ADVm\n1gjcBZQCuPu9wJ3AXOCLZgaQDEbwzAceCsriwDfd/ZExOIZQmpZZRCRcPqN6bhli/e8Dvx9Svh+4\n/Nw9RERkIkX2zl318YuIhIts8IuISLjIBr8a/CIi4SIX/JqVU0Qkt8gFfz99AIiIhIpu8IuISKjI\nBb8P+FNERM4WueAXEZHcohf8QVNfXfwiIuGiF/wiIpJT5IL/jT5+NflFRMJELvhFRCS3yAV/3w1c\n6uMXEQkXueAXEZHcIhf8auiLiOQWueDvow8AEZFwkQt+9e2LiOQWueDvow8AEZFwkQ1+EREJF7ng\n77txSzdwiYiEyyv4zWyTmTWZ2c5B1puZfc7MGsxsh5ldlbXuVjPbGzxuLVTFRURkZPJt8X8NWJNj\n/VpgZfDYAHwJwMzmAHcBVwOrgbvMbPZIK5sP17zMIiI55RX87v4k0JJjk3XAv3vGs8AsM1sAvAf4\nkbu3uPsp4Efk/gAREZExVqg+/kXA4azlxqBssPJzmNkGM6s3s/rm5uYRV6Svxa8Gv4hIuElzcdfd\nN7p7nbvX1dbWTnR1REQiq1DBfwRYkrW8OCgbrHzMuQbyi4iEKlTwbwZ+Oxjdcw1wxt2PAo8CN5nZ\n7OCi7k1BmYiITJB4PhuZ2X3AO4AaM2skM1KnFMDd7wW2ADcDDUAn8LvBuhYz+xSwNXipu90910Xi\nUdO0zCIiueUV/O5+yxDrHfjYIOs2AZuGXzURERkLk+bibqGpwS8iEi5ywa/AFxHJLXLB30d9/CIi\n4SIX/Ap8EZHcIhf8fTQ7p4hIuMgFvwJfRCS3yAV/H3X5iIiEi1zwK/BFRHKLXPCLiEhukQt+NfhF\nRHKLXPCLiEhukQv+/h9iceevHtjBb331uYmtkIjIJJPXJG1T1bfqDw+9kYhIkYlci79PR29qoqsg\nIjIpRTD4M309D2xrnOB6iIhMThEMfhERySVywa8buEREcotc8IuISG6RC341+EVEcotc8IuISG55\nBb+ZrTGzPWbWYGa3h6z/rJltDx6vmtnprHWprHWbC1n5MK5OfhGRnIa8gcvMYsAXgBuBRmCrmW12\n95f7tnH3T2Rt/8fAlVkv0eXuVxSuyiIiMhr5tPhXAw3uvt/de4H7gXU5tr8FuK8QlRsJtfdFRHLL\nJ/gXAdlzHzQGZecws2XACuDxrOIKM6s3s2fN7NcHexMz2xBsV9/c3JxHtUREZCQKfXF3PfCAu2fP\nl7DM3euADwL/bGYXhO3o7hvdvc7d62prawtcLRER6ZNP8B8BlmQtLw7KwqxnQDePux8J/twP/JSz\n+/8LTtd2RURyyyf4twIrzWyFmZWRCfdzRueY2UXAbODnWWWzzaw8eF4DvA14eeC+IiIyfoYc1ePu\nSTP7OPAoEAM2ufsuM7sbqHf3vg+B9cD9fvZ4ylXAl80sTeZD5m+zRwONBQ3nFBHJLa/5+N19C7Bl\nQNmdA5b/d8h+zwCXjaJ+IiJSYJG7c1ftfRGR3CIX/CIiklv0gl9NfhGRnKIX/CIiklPkgl8NfhGR\n3CIX/CIiklvkgl/j+EVEcotc8IuISG4KfhGRIhO54FdHj4hIbpELfhERyS1ywa9ruyIiuUUu+EVE\nJLfIBb+rl19EJKdIBX9TazfdifREV0NEZFKLVPCv/sxjoeXffWGwX4oUESk+kQr+wfzZt7bTnUgN\nvaGISBEoiuAXEZE3FE3wpzXOU0QEKKrgn+gaiIhMDnkFv5mtMbM9ZtZgZreHrP8dM2s2s+3B4/ez\n1t1qZnuDx62FrPxwqMUvIpIRH2oDM4sBXwBuBBqBrWa22d1fHrDpt9z94wP2nQPcBdSRmUZnW7Dv\nqYLUfhhcozxFRID8WvyrgQZ33+/uvcD9wLo8X/89wI/cvSUI+x8Ba0ZW1dFRi19EJCOf4F8EHM5a\nbgzKBvpvZrbDzB4wsyXD3Bcz22Bm9WZW39zcnEe1hkfBLyKSUaiLu/8FLHf3t5Bp1f/bcF/A3Te6\ne52719XW1haoWm/QxV0RkYx8gv8IsCRreXFQ1s/dT7p7T7D4FeCt+e47XvSTjCIiGfkE/1ZgpZmt\nMLMyYD2wOXsDM1uQtfg+4JXg+aPATWY228xmAzcFZePuY998fiLeVkRk0hlyVI+7J83s42QCOwZs\ncvddZnY3UO/um4E/MbP3AUmgBfidYN8WM/sUmQ8PgLvdvWUMjmNIWw+O+0AiEZFJacjgB3D3LcCW\nAWV3Zj2/A7hjkH03AZtGUUcRESmgorlzV0REMooq+FMa2iMiUlzBn0jp9l0RkaIK/qRa/CIiRRb8\navGLiBRX8CdSavGLiBRV8CfTavGLiBRX8KvFLyISzeB//5WhE4BqVI+ICBEN/qVzpoeWb/j6NuoP\nTsiMESIik0Ykg3+wDp2GpnY+8o1t41oXEZHJJpLBT44pmMvjsXGsiIjI5BPJ4M91n1Z5PJKHLCKS\nt0imoA/a2QNlCn4RKXKRTMFcP7alFr+IFLtIpmCurp4XG8/w7frDg28gIhJxkQz+XF09AN/aquAX\nkeIVyeAfIvc1S6eIFLVIBv9QsZ5W8ItIEcsr+M1sjZntMbMGM7s9ZP2fm9nLZrbDzB4zs2VZ61Jm\ntj14bC5k5QejYBcRGdyQP7ZuZjHgC8CNQCOw1cw2u/vLWZu9ANS5e6eZfRT4e+C/B+u63P2KAtc7\np6FivzRm41IPmVhNrd2s/sxjAFx3YQ0XnVfFz/ad5GR7D1+5tY5LFs4kVqJ/C1J88mnxrwYa3H2/\nu/cC9wPrsjdw95+4e2ew+CywuLDVHJ5cwzkBnj90mtX3/JhHdh4dnwrJuNvy0tH+0Ad4uuEEX3n6\nAK8cbaWprYf3ff5n/PV3d+JD/WMRiaB8gn8RkD0MpjEoG8xtwA+ylivMrN7MnjWzXx9BHYfNcW5f\ne1HObZraevjIN54fj+rIGGnrTvDayQ6e2XeCrz/7GtteO0VTazfP7DvBH/3H0Of2vl8c4pr/8xhP\n7W3mxy8fH4cai0wOQ3b1DIeZfRioA67PKl7m7kfM7HzgcTN7yd33hey7AdgAsHTp0lHVwx0+cv0F\n/KzhBE/tPZFz268+fYDbrlsxqveT8eXu/NV/7uDb9Y2jfq3jrT381ld/AWSm8/70r19KZXlB/1uI\nTDr5/As/AizJWl4clJ3FzG4APglc7+49feXufiT4c7+Z/RS4Ejgn+N19I7ARoK6ublTfv/u+vvck\nhp5//1Pff5kPXLWYtDtVFXHisUgOdJryOnuTHGrp5KEXjvDlJ/aPyXs89MIRHtl5jE/cuJJDLZ1M\nK42xfvVSLqidMSbvJ8UtlfYJu8aUT/BvBVaa2Qoygb8e+GD2BmZ2JfBlYI27N2WVzwY63b3HzGqA\nt5G58Dum+j417nn/pdz42SeH3P7yu38IwHvfsoB7ghZfqT4AJlxDUztdvSm2Hz7F//rernF5z65E\nis9s2d2//H+fOsDs6aV88w+uYdWC6nGpg0xtfQ1Ps3NDvamtm83bX+fTD78CwHsvW8AnbnwTS+ZM\nG9eZg4cMfndPmtnHgUeBGLDJ3XeZ2d1AvbtvBv4BmAF8JzjYQ+7+PmAV8GUzS5O5nvC3A0YDjYm+\n63Ur51dx74evyrsv/+EdR3l4R+aC78G/fe9YVU9CPNNwglePt7G3qZ1n9p3kwImOia5Sv1OdCdb+\ny1N86UNXsfayBRNdHZmkEqk0d35vJ/f9InNJtCxewoa3n0/NjDJ2H2vj/pAZAx5+6SgPv/TGIJP9\nn7mZknH4FpBXZ6a7bwG2DCi7M+v5DYPs9wxw2WgqOBLprJEaN6yazx9ef/6wuweW3/4w//Pmi/jg\n1cuoLIuFfnpPhKf3nuC/XnydI6e7eNuFNay99DyW11QCmZbGZKnnUJKpNI2nutiy8yhf/Mk+2nuS\nE12lIX30P55n6ydvoLaqfKKrIuMonXZ6U2liJRbaE5BMpfnFgRY++JXnzirvTab5/E8ahvVex1q7\nWThr2qjqm49IXsXKvkAQj5Vwx9pVI+oX/syW3f1f+1fUVPIbdYt555vncdF5VWMasGe6EnT0JDnd\nmWB+dTkzKuIcPd3N5x7fy4PPv3F55emGE/zdI7uprojT2p2kLFbCpYuq+cBbl/D2lTUsGeQnKCdC\nZ2+S5187zc7Xz7Cj8TRbXjo20VUakV+658d86UNXsWpBNcvmTp8yH7SSv/aeJC3tvex6/Qzf2dbI\n47v7e6/51bcs4PyaSp4/dJqnG3IPHBmJgyc7FPzDdc/7L+WTD+0MHcd/8YJqVs6fwfe2vz6i1z5w\nooO/f2QPf//InrPK33XRPC5ZWM3CWdNYPreSyvIY5fEY8Zjh7pTFYsycXkp5vITSWAnuTokZnYlU\n/zeTdNp59Xg7j+9uYtPTB+gd5o/Ct3ZnWsu9qTTPHzrN84dOAzBreinvevM85s+soL07ycJZ07hi\nySwuXlDNzOmlg76eu9PRm2LnkTMcaumkpaOXZXOms2pBdf+3i8F0J1IcPNnB7qNttPckOdney+YX\nj7CvefJ03YzWR7OGin74mqXcdt35VJbHqJ1Rfs4HQb7fwnYfa6UiHmP29LKc50YGt/HJfZxs72Xl\n/CrevrKG+dUV/eu6EylOdvRyoLmDV4+3MaM8zmWLZ7Jw1jRmTislmUpz7xP7+McfvprzPb6/Y2zv\n/Wnp6B3T1+8TqeA3Mv/Bwm7K2fKnbwcYcfAP5vHdTWe1CCaT050JHnzhnAFYALx12WzeddE8Vi2o\nYtb0Mrp6U7x+uovjrd3c+8T+Qbteblg1n3evmseiWdPY+foZfrq7mZMdmUFcUQr3fH3j2UN849lD\n/ctL50znw9cs5bJFszjZ0cNffPtFLqidwe+8bTm/WbeERCpN/cFTzJxWypzKMk609/A3/7WLrQdP\n9b/GRedVUbd8Nl29aS5dVM0Nq+ZPqm9vEyWVdl493sahlk6uf1Mt5fESzIwjp7t47+ee4nRn4qzt\n376yhp5kphtmqmjtGp8uT5uMdy7W1dV5fX39sPd78PlG/vzbL7L20vP40offGrrNm/76B/Qmh9ei\nFimE0piRSI3s/9tVS2dx6y8vZ05lGbOnl/Gm+VVT6tfk3J1EyvnBzqNUTyvlkoXVzKt6o0Xek0xx\nor2XlvZeTnT00Nad5M3zq3jT/Bk0tfXwxKvN/OUDO856zdnTS3nL4lk88WrzeB/OmLlj7UX84fUX\njGhfM9vm7nX5bBupFv+1F8wFYOncwVtH3/2jt3Hz554aryqJ9Btp6ANBF972s8r+4O0r+OULa5hW\nGuOXls85a0x4TzJFW9AFOKM8TkVp+FDB3mSaRCrNtNIYJSVGTzLFc/tb+MHOY/ziwEn2NXdQM6OM\nGy8+j3VXLOTqFXNydl01tXbz01ebeWrvCaoq4pxfU0lHT4oHX2jktZOdZ21bHi/h3avmsftYG/tH\n8G3xVGciUqEP9J+zsRapFj/A4ZZO5lWX5xwTu7+5nXf/0xNDzukjMtVUVcRDw2NeVTmV5XGqKuIs\nmT2d6mml7D7WygvB9aDhWHPJeay97DwuWVjNzGll7G1qY/fRNv7t5wfPCXcZnt++dhl3r7t0RPsW\nbYsfyKsv9PzaGey752Y+9fDLfPO5Q/So60ciYrAWY1NbD7RlrsXsaDwzqvd4ZNcxHtk1NUdlTXad\nvalxeZ+p00lYYCUlxl2/dgl7Pr2Wb224hnVXLCRWYsQ1Ta+ITJCuxPgEf+Ra/CNx9flzufr8ufzL\n+iv7yzp6kjS39bDr9VZ+/MpxHhpkdIyISKF0j1OLX8E/iMryOJXlcZbXVPLetyzgn37zcprbejjT\nleBEey+HT3Wy+2gbJzt62N/cwZ7jbRotJCKjohb/JGNmzKuuYF51BSvnw7XMDd3O3elOpEmm06Q9\ns9ybStOTSGOWmUeoJxhJ0ZVIETM7a8RFKu20dieYW1nG3BnlxMxIpNO0dPTiDgtnVTCtNHbWLKI9\nyRSlJSWYvTExVCKVJl5ioSMw2roT9CbT9CQzdTje2k1VeSmxEuNMV4Ijp7vY19xOMpWmqqKUtu4E\nWw+eYvvh08HfxdA/diMiw6fgn6LMjGllMTLz2RVOzYzB54cJG8GUa3bRqoqz7wwd6bTDyVTmw61v\nPHlvMvMB1Rp8sJQH5V2JFM1tPTS19dDWnaCyPM7cyjKOt/bw+ukuOntT/UMRtx8+zc4jZ0jqd5Ol\nCHWpq0cmu4G/XVAWL+G8mRWcN7NikD1GpyeZor07ycGTHbjDglnTqAju3uxNpkm7Ew++tcRKjBkV\ncWoqy2np7KU7kcI9c1G/LFZCVUWc1u4EZbESYiVG2uFke+aazqGWTo6d6WZ6eYwLamcwv7qCnkSK\nrkTmJqOm1m6agxEyNVXlTCuNkUo77T1Jjrd203iqi1eOtlJixkULqrhkYTXtPZk7o2eUx5leFqOj\nN0VjSyedvSmOtXaPyd+XTD3jNcJQwS9TRnk8RvmMGHNzfPsBmFd99gfPYN+WBt7UNHNaKedPsh9d\nSabSJNNOdyJFS0fvOcP93GHOjDJKY0ZPIk1PMkV3Is2ZrgTTymKUx0tIpTPdj61dCXqDLsCuRIre\nZGbGyXishDfPr2LmtFK6EinOdCVo6ejBzIiZsb+5nYMnO+lNpamqiINnfivh5/tP9tdn2dzpXLZo\nJml3Xjx8hiOnuyiLlVBeWnLOENPM3cellMZKqCyPU10Rx8zoTqSYOa2Uqoo4ZUG9z3QlSKac2dPL\nqKqIYwZHz3TT0NTO7mNt43YeIDNR46WLZnL0dBf1r50aeocRUItfRIjHSojHMh9Ss6aXTUgdfuVN\ntaN+jbGaMjwRTGhoZO6M7k6kmFYW678O1Z1IUV1R2j/HfSKV7r++FTMjmU6TcscwSmPGjPL8f4Uv\nnXb2n+igqbWb1u4k8RKjqiJOVUXpGxMzlpRgJZlAL42VUBYvobMnSWt3gua2Xo6c7qK1K0FXIsWh\nk53jNuW3gl9ExtxYTV+dfS0rHiO4vvaGgd/qSmMlwTeN0b93SYlx4bwZXDhv6G+J1VnX1WaUx5lX\nXcGF80Zfh5Eq2hu4RESKlYJfRKTIKPhFRIqMgl9EpMjkFfxmtsbM9phZg5ndHrK+3My+Fax/zsyW\nZ627IyjfY2bvKVzVRURkJIYMfjOLAV8A1gIXA7eY2cUDNrsNOOXuFwKfBf4u2PdiYD1wCbAG+GLw\neiIiMkHyafGvBhrcfb+79wL3A+sGbLMO+Lfg+QPAuy0zfmsdcL+797j7AaAheD0REZkg+QT/IuBw\n1nJjUBa6jbsngTPA3Dz3BcDMNphZvZnVNzdH6+fUREQmk0lzA5e7bwQ2AphZs5m9NsKXqgFOFKxi\nU4OOuTjomKNvNMe7LN8N8wn+I8CSrOXFQVnYNo1mFgdmAifz3Pcc7j7ie8TNrD7f352MCh1zcdAx\nR994HW8+XT1bgZVmtsLMyshcrN08YJvNwK3B8w8Aj3vmV9w3A+uDUT8rgJXALwpTdRERGYkhW/zu\nnjSzjwOPkplkfpO77zKzu4F6d98MfBX4upk1AC1kPhwItvs28DKQBD7m7uMz/ZyIiITKq4/f3bcA\nWwaU3Zn1vBv4jUH2vQe4ZxR1HK6N4/hek4WOuTjomKNvXI7XXL+hJyJSVDRlg4hIkVHwi4gUmcgE\n/1DzCU1VZrbEzH5iZi+b2S4z+9OgfI6Z/cjM9gZ/zg7Kzcw+F/w97DCzqyb2CEbOzGJm9oKZfT9Y\nXhHMBdUQzA1VFpQPOlfUVGJms8zsATPbbWavmNm1UT/PZvaJ4N/1TjO7z8wqonaezWyTmTWZ2c6s\nsmGfVzO7Ndh+r5ndGvZe+YpE8Oc5n9BUlQT+wt0vBq4BPhYc2+3AY+6+EngsWIbM38HK4LEB+NL4\nV7lg/hR4JWv574DPBnNCnSIzRxQMMlfUFPQvwCPufhFwOZljj+x5NrNFwJ8Ade5+KZlRg+uJ3nn+\nGpm5yrIN67ya2RzgLuBqMtPe3NX3YTEi7j7lH8C1wKNZy3cAd0x0vcboWL8H3AjsARYEZQuAPcHz\nLwO3ZG3fv91UepC52e8x4F3A98n8rOoJID7wnJMZanxt8DwebGcTfQzDPN6ZwIGB9Y7yeeaNKV3m\nBOft+8B7oniegeXAzpGeV+AW4MtZ5WdtN9xHJFr8DGNOoKks+Gp7JfAcMN/djwarjgHzg+dR+bv4\nZ+AvgXSwPBc47Zm5oODs4xpsrqipZAXQDPy/oHvrK2ZWSYTPs7sfAf4ROAQcJXPethHt89xnuOe1\noOc7KsEfeWY2A/hP4M/cvTV7nWeaAJEZl2tmvwo0ufu2ia7LOIoDVwFfcvcrgQ7e+PoPRPI8zyYz\ng+8KYCFQybldIpE3Eec1KsE/ojmBpgozKyUT+v/h7g8GxcfNbEGwfgHQFJRH4e/ibcD7zOwgmWnA\n30Wm/3tWMBcUnH1c/cc8YK6oqaQRaHT354LlB8h8EET5PN8AHHD3ZndPAA+SOfdRPs99hnteC3q+\noxL8+cwnNCWZmZGZEuMVd/8e/5kZAAABJElEQVSnrFXZ8yPdSqbvv6/8t4PRAdcAZ7K+Uk4J7n6H\nuy929+VkzuXj7v4h4Cdk5oKCc485bK6oKcPdjwGHzezNQdG7yUx1EtnzTKaL5xozmx78O+875sie\n5yzDPa+PAjeZ2ezgm9JNQdnITPRFjwJePLkZeBXYB3xyoutTwOO6jszXwB3A9uBxM5m+zceAvcCP\ngTnB9kZmhNM+4CUyIyYm/DhGcfzvAL4fPD+fzCR/DcB3gPKgvCJYbgjWnz/R9R7hsV4B1Afn+rvA\n7KifZ+BvgN3ATuDrQHnUzjNwH5lrGAky3+xuG8l5BX4vOPYG4HdHUydN2SAiUmSi0tUjIiJ5UvCL\niBQZBb+ISJFR8IuIFBkFv4hIkVHwi4gUGQW/iEiR+f9EURwQtSnjkQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type KhaledNetRNN33. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4d12ccc1c5fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-3352eb890ccb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mloss_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RNN400\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/gdrive/My Drive/APS360 Project/Data/Model Checkpoints/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.KhaledNetRNN33'>: it's not the same object as __main__.KhaledNetRNN33"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp2wHhoJFhHt",
        "colab_type": "code",
        "outputId": "23cec2ce-caf1-4134-8f01-08093a4908de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "one = torch.tensor([[1.0,1.0,1.0],[2.0,2.0,2.0]])\n",
        "t1 = torch.tensor([0.1,0.1,0.1])\n",
        "t2 = torch.tensor([3.0,2.0,2.0])\n",
        "\n",
        "t = torch.stack((t1,t2))\n",
        "t*one"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1000, 0.1000, 0.1000],\n",
              "        [6.0000, 4.0000, 4.0000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-QYO8fGFhwT",
        "colab_type": "code",
        "outputId": "58e049df-3a0a-483b-e9db-3c8dccb3c3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqBUq31-PdTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Added get_loss function and considered validation loader when calculating loss and graphing the training curve\n",
        "\n",
        "def chunkate(tensor, number_of_chunks):\n",
        "  padded_tensor = torch.zeros(number_of_chunks * 16000).cuda()\n",
        "  padded_tensor[:len(tensor)] = tensor\n",
        "  chunks = []\n",
        "  for chunk in range(number_of_chunks):\n",
        "    chunks.append(padded_tensor[chunk * 16000: (chunk+1) *16000].cuda())\n",
        "  return chunks\n",
        "\n",
        "class BaselineRNN33(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetRNN33, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetRNN3\"\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True).cuda()\n",
        "    self.fc = nn.Linear(hidden_size, input_size*2).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(1, x.size(0), self.hidden_size).cuda()\n",
        "    x.cuda()\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output\n",
        "\n",
        "  \n",
        "def get_loss(model, data_loader):\n",
        "  losses = []\n",
        "  criterion = nn.MSELoss()\n",
        "  for batchData, (truth1, truth2) in data_loader:  \n",
        "    batch_loss = 0    \n",
        "    for i in range(len(batchData)):                \n",
        "      inputs = SPHFile(batchData[i]).content\n",
        "      truths1 = SPHFile(truth1[i]).content\n",
        "      truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\")).cuda()\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\")).cuda()\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\")).cuda()\n",
        "\n",
        "      number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "\n",
        "      input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "      truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "      truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "\n",
        "      for j in range(number_of_chunks):\n",
        "        pred = model(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
        "        loss1 = criterion(pred[:16000].float() * input_chunks[j].float(), truth1_chunks[j].float())\n",
        "        loss2 = criterion(pred[16000:].float() * input_chunks[j].float(), truth2_chunks[j].float())\n",
        "        batch_loss += loss1 + loss2\n",
        "    losses.append(batch_loss.item())\n",
        "  average_loss = np.array(losses).mean()\n",
        "  return average_loss\n",
        "\n",
        "  \n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4): \n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  loss = []\n",
        "  val_loss = [] # an array of validation losses\n",
        "  loss_counter = 0\n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:  \n",
        "      batch_loss = 0    \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content\n",
        "        truths1 = SPHFile(truth1[i]).content\n",
        "        truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "        t_inputs = torch.tensor(inputs.astype(\"int16\")).cuda()\n",
        "        t_truths1 = torch.tensor(truths1.astype(\"int16\")).cuda()\n",
        "        t_truths2 = torch.tensor(truths2.astype(\"int16\")).cuda()\n",
        "        \n",
        "        number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "        \n",
        "        input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "        truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "        truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "        \n",
        "        for j in range(number_of_chunks):\n",
        "          pred = model(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)\n",
        "          loss1 = criterion(pred[:16000].float() * input_chunks[j].float(), truth1_chunks[j].float())\n",
        "          loss2 = criterion(pred[16000:].float() * input_chunks[j].float(), truth2_chunks[j].float())\n",
        "          batch_loss += loss1 + loss2\n",
        "          \n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      loss.append(batch_loss)\n",
        "      \n",
        "      # Calculating validation loss using \"get_loss\" function\n",
        "      val_loss1 = get_loss(model, valid_loader)\n",
        "      val_loss.append(val_loss1)\n",
        "      \n",
        "      loss_counter += 1\n",
        "      \n",
        "#       if loss_counter % 10 == 0:\n",
        "#         print(\"Batch \", loss_counter, \": \", batch_loss.item())\n",
        "      if loss_counter % 500 == 0:\n",
        "        plt.title(\"Training Curve\")\n",
        "        plt.plot(loss, label=\"Train\")\n",
        "        plt.plot(val_loss, label=\"Validation\")\n",
        "        plt.xlabel(\"Counter\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "      if loss_counter % 1000 == 0:\n",
        "        model_name = \"RNN400\" + str(loss_counter)\n",
        "        torch.save(model.state_dict, \"/content/gdrive/My Drive/APS360 Project/Data/Model Checkpoints/\"+model_name)\n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Train loss:\", batch_loss, \"Validation loss:\", val_loss1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HY7Q3pjE6t2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelA = BaselineRNN33(16000,1000)\n",
        "# train(modelA, train_loader, val_loader, num_epochs=500, learning_rate=1e-3) #train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nGKNNBdr9OV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#proper batching \n",
        "def get_loss(model, data_loader):\n",
        "  losses = []\n",
        "  criterion = nn.MSELoss()\n",
        "  for batchData, (truth1, truth2) in train_loader:  \n",
        "    batch_loss = 0 \n",
        "\n",
        "    batch_inputs = []\n",
        "    batch_truths1 = []\n",
        "    batch_truths2 = []\n",
        "\n",
        "    for i in range(len(batchData)):                \n",
        "      inputs = SPHFile(batchData[i]).content\n",
        "      truths1 = SPHFile(truth1[i]).content\n",
        "      truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "      t_inputs = torch.tensor(inputs.astype(\"int16\")).cuda()\n",
        "      t_truths1 = torch.tensor(truths1.astype(\"int16\")).cuda()\n",
        "      t_truths2 = torch.tensor(truths2.astype(\"int16\")).cuda()\n",
        "\n",
        "      number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "\n",
        "      input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "      truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "      truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "\n",
        "      for j in range(number_of_chunks):\n",
        "        batch_inputs.append(input_chunks[j].float().cuda())\n",
        "        batch_truths1.append(truth1_chunks[j].float().cuda())\n",
        "        batch_truths2.append(truth2_chunks[j].float().cuda())\n",
        "\n",
        "    batch_inputs = torch.stack(batch_inputs).cuda()\n",
        "    batch_truths1 = torch.stack(batch_truths1).cuda()\n",
        "    batch_truths2 = torch.stack(batch_truths2).cuda()\n",
        "\n",
        "    pred = model(batch_inputs.unsqueeze(1))\n",
        "\n",
        "    loss1 = criterion(pred[:, :16000].float() * batch_inputs, batch_truths1)\n",
        "    loss2 = criterion(pred[:, 16000:].float() * batch_inputs, batch_truths2)\n",
        "\n",
        "    batch_loss = (loss1 + loss2)/len(pred)\n",
        "    losses.append(batch_loss.item())\n",
        "    break\n",
        "  average_loss = np.array(losses).mean()\n",
        "  return average_loss\n",
        "\n",
        "\n",
        "def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4): \n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        \n",
        "  loss = []\n",
        "  val_loss = [] # an array of validation losses\n",
        "  loss_counter = 0\n",
        "  for epoch in range(num_epochs): \n",
        "    for batchData, (truth1, truth2) in train_loader:  \n",
        "      batch_loss = 0 \n",
        "      \n",
        "      batch_inputs = []\n",
        "      batch_truths1 = []\n",
        "      batch_truths2 = []\n",
        "      \n",
        "      for i in range(len(batchData)):                \n",
        "        inputs = SPHFile(batchData[i]).content\n",
        "        truths1 = SPHFile(truth1[i]).content\n",
        "        truths2 = SPHFile(truth2[i]).content\n",
        "\n",
        "        t_inputs = torch.tensor(inputs.astype(\"int16\")).cuda()\n",
        "        t_truths1 = torch.tensor(truths1.astype(\"int16\")).cuda()\n",
        "        t_truths2 = torch.tensor(truths2.astype(\"int16\")).cuda()\n",
        "        \n",
        "        number_of_chunks = math.ceil(max(len(t_inputs), len(t_truths1), len(t_truths2))/16000)\n",
        "        \n",
        "        input_chunks = chunkate(t_inputs, number_of_chunks)\n",
        "        truth1_chunks = chunkate(t_truths1, number_of_chunks)\n",
        "        truth2_chunks = chunkate(t_truths2, number_of_chunks)\n",
        "        \n",
        "#         print(len(inputs))\n",
        "#         print(number_of_chunks)\n",
        "        \n",
        "        for j in range(number_of_chunks):\n",
        "          batch_inputs.append(input_chunks[j].float().cuda())\n",
        "          batch_truths1.append(truth1_chunks[j].float().cuda())\n",
        "          batch_truths2.append(truth2_chunks[j].float().cuda())\n",
        "      \n",
        "      batch_inputs = torch.stack(batch_inputs).cuda()\n",
        "      batch_truths1 = torch.stack(batch_truths1).cuda()\n",
        "      batch_truths2 = torch.stack(batch_truths2).cuda()\n",
        "      \n",
        "      pred = model(batch_inputs.unsqueeze(1))\n",
        "      \n",
        "#       print(\"Done prediction\")\n",
        "      loss1 = criterion(pred[:, :16000].float() * batch_inputs, batch_truths1)\n",
        "      loss2 = criterion(pred[:, 16000:].float() * batch_inputs, batch_truths2)\n",
        "      \n",
        "      batch_loss = (loss1 + loss2)/len(pred)\n",
        "#       print(\"Got loss\")\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "      loss.append(batch_loss)\n",
        "      \n",
        "#       print(\"Done backward pass\")\n",
        "      \n",
        "      # Calculating validation loss using \"get_loss\" function\n",
        "      val_loss1 = get_loss(model, valid_loader)\n",
        "#       print(\"Got val loss\")\n",
        "      val_loss.append(val_loss1)\n",
        "      print(\"Train loss:\", batch_loss.item(), \"Validation loss:\", val_loss1)\n",
        "      loss_counter += 1\n",
        "      \n",
        "#       if loss_counter % 10 == 0:\n",
        "#         print(\"Batch \", loss_counter, \": \", batch_loss.item())\n",
        "      if loss_counter % 10 == 0:\n",
        "        plt.title(\"Training Curve\")\n",
        "        plt.plot(loss, label=\"Train\")\n",
        "        plt.plot(val_loss, label=\"Validation\")\n",
        "        plt.xlabel(\"Counter\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "      if loss_counter % 1000 == 0:\n",
        "        model_name = \"RNN400\" + str(loss_counter)\n",
        "        torch.save(model.state_dict, \"/content/gdrive/My Drive/APS360 Project/Data/Model Checkpoints/\"+model_name)\n",
        "    if epoch % 1 == 0:\n",
        "      print(\"Epoch loss:\", batch_loss.item(), \"Validation loss:\", val_loss1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S95xo9-ZWTZk",
        "colab_type": "code",
        "outputId": "bef565c7-82c2-4bee-c5ca-d03d14f7adc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Train/\")\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "print(len(train_set))\n",
        "\n",
        "val_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Val/\")\n",
        "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "print(len(val_set))\n",
        "\n",
        "test_set = DataFile(\"/content/gdrive/My Drive/APS360 Project/Data/Test/\")\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=0)\n",
        "print(len(test_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5975\n",
            "3984\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H536WRo0yI5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline = BaselineRNN33(16000,100)\n",
        "train(baseline, train_loader, val_loader, num_epochs=10, learning_rate=1e-3) #train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74r9ujH50GhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = BaselineRNN33(16000,1000)\n",
        "train(actual, train_loader, val_loader, num_epochs=10, learning_rate=1e-3) #train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqQH9Gi5y_ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "for x in train_loader:\n",
        "  inputs = torch.tensor(SPHFile(x[0][0]).content.astype(\"int16\"))  #get the file\n",
        "  i+=1\n",
        "  if i == 1:\n",
        "    break\n",
        "\n",
        "number_of_chunks = math.ceil(len(inputs)/16000)\n",
        "input_chunks = chunkate(inputs, number_of_chunks)\n",
        "\n",
        "speaker1 = torch.tensor([])\n",
        "speaker2 = torch.tensor([])\n",
        "\n",
        "for j in range(number_of_chunks):\n",
        "  masks = baseline(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)  \n",
        "  out_chunk1 = input_chunks[j].squeeze() * masks.squeeze()[:16000]  #multiply the first half of the mask by the overlay file\n",
        "  out_chunk2 = input_chunks[j].squeeze() * masks.squeeze()[16000:]  #multiply the second half of the mask by the overlay file\n",
        "  speaker1 = torch.cat((speaker1, out_chunk1.cpu()))\n",
        "  speaker2 = torch.cat((speaker2, out_chunk2.cpu()))\n",
        "\n",
        "\n",
        "export_this1 = speaker1.detach().numpy() #ugh\n",
        "export_this2 = speaker2.detach().numpy() #not this again\n",
        "\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/testBaseline1.wav\", export_this1.astype(\"int16\"), 16000)  #output the first dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/testBaseline2.wav\", export_this2.astype(\"int16\"), 16000)  #output the second dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/testBaseline3.wav\", inputs.detach().numpy().astype(\"int16\"), 16000) #output the original file to make sure \n",
        "                                                                                                                        #i wasnt doing anything wrong\n",
        "  \n",
        "for j in range(number_of_chunks):\n",
        "  masks = actual(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)  \n",
        "  out_chunk1 = input_chunks[j].squeeze() * masks.squeeze()[:16000]  #multiply the first half of the mask by the overlay file\n",
        "  out_chunk2 = input_chunks[j].squeeze() * masks.squeeze()[16000:]  #multiply the second half of the mask by the overlay file\n",
        "  speaker1 = torch.cat((speaker1, out_chunk1.cpu()))\n",
        "  speaker2 = torch.cat((speaker2, out_chunk2.cpu()))\n",
        "\n",
        "\n",
        "export_this1 = speaker1.detach().numpy() #ugh\n",
        "export_this2 = speaker2.detach().numpy() #not this again\n",
        "\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/testActual1.wav\", export_this1.astype(\"int16\"), 16000)  #output the first dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/testActual2.wav\", export_this2.astype(\"int16\"), 16000)  #output the second dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/testActual3.wav\", inputs.detach().numpy().astype(\"int16\"), 16000) #output the original file to make sure \n",
        "                                                                                                                        #i wasnt doing anything wrong"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev-J-vFiA2Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KhaledNetRNN4(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetRNN4, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetRNN4\"\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True, num_layers=5).cuda()\n",
        "    self.fc = nn.Linear(hidden_size, input_size*2).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(5, x.size(0), self.hidden_size).cuda()\n",
        "    x.cuda()\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMspjaTkTfmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deep_model = KhaledNetRNN4(16000,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "610NDDRpTiV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(deep_model, train_loader, val_loader, num_epochs=1, learning_rate=1e-4) #train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESwPeVU3TlAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KhaledNetGRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(KhaledNetGRU, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.name = \"KhaledNetGRU\"\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, batch_first=True, num_layers = 2).cuda()\n",
        "    self.fc = nn.Linear(hidden_size, input_size*2).cuda()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(2, x.size(0), self.hidden_size).cuda()\n",
        "    x.cuda()\n",
        "    output, _ = self.rnn(x, h0)\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ZKG8YpJhFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GRU_model = KhaledNetGRU(16000,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHf2TajfK3Jq",
        "colab_type": "code",
        "outputId": "7093a09a-45b3-4fb2-ba29-1dc9f0e9a248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "train(GRU_model, train_loader, val_loader, num_epochs=1000, learning_rate=1e-2) #train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9cccdf761f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFhzNQIcK9bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "for x in train_loader:\n",
        "  inputs = torch.tensor(SPHFile(x[0][0]).content.astype(\"int16\"))  #get the file\n",
        "  i+=1\n",
        "  if i == 1:\n",
        "    break\n",
        "\n",
        "number_of_chunks = math.ceil(len(inputs)/16000)\n",
        "input_chunks = chunkate(inputs, number_of_chunks)\n",
        "\n",
        "speaker1 = torch.tensor([])\n",
        "speaker2 = torch.tensor([])\n",
        "\n",
        "for j in range(number_of_chunks):\n",
        "  masks = GRU_model(input_chunks[j].float().unsqueeze(0).unsqueeze(0)).squeeze(0)  \n",
        "  out_chunk1 = input_chunks[j].squeeze() * masks.squeeze()[:16000]  #multiply the first half of the mask by the overlay file\n",
        "  out_chunk2 = input_chunks[j].squeeze() * masks.squeeze()[16000:]  #multiply the second half of the mask by the overlay file\n",
        "  speaker1 = torch.cat((speaker1, out_chunk1.cpu()))\n",
        "  speaker2 = torch.cat((speaker2, out_chunk2.cpu()))\n",
        "\n",
        "\n",
        "export_this1 = speaker1.detach().numpy() #ugh\n",
        "export_this2 = speaker2.detach().numpy() #not this again\n",
        "\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/GRU1.wav\", export_this1.astype(\"int16\"), 16000)  #output the first dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/GRU2.wav\", export_this2.astype(\"int16\"), 16000)  #output the second dude\n",
        "wavio.write(\"/content/gdrive/My Drive/APS360 Project/Data/GRU3.wav\", inputs.detach().numpy().astype(\"int16\"), 16000) #output the original file to make sure \n",
        "                                                                                                                        #i wasnt doing anything wrong"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uMzwmt-VWHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}